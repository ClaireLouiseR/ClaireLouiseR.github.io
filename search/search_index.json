{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Claire Russell Data Science Portfolio <p>I\u2019m a Berlin-based Data Analyst with a Master of Applied Data Analytics (ANU) specialising in computer science.</p> <p>This site shares selected projects and notebooks while I build out new work.</p> <ul> <li>Email: clairelourussell@gmail.com</li> <li>GitHub: @ClaireLouiseR</li> <li>LinkedIn: Claire Russell</li> </ul> <p>I\u2019m currently repurposing a few uni projects as portfolio pieces\u2014more coming soon.</p>"},{"location":"#projects","title":"Projects","text":"Indo-Pacific Diplomacy (Networks) <p>Network science on Lowy indices to surface brokerage opportunities.</p> Facial Emotion Recognition <p>Facial emotion classifier trained in Colab; SVM vs NN comparison and evaluation.</p>"},{"location":"projects/","title":"Projects","text":"<ul> <li> Leveraging Network Science to Enhance Australia\u2019s Indo-Pacific Diplomacy <p>Network measures (betweenness, clustering, weighted degrees) on Lowy datasets to surface brokerage opportunities.</p> <p>View project \u2192</p> </li> </ul>"},{"location":"projects/emotion-recognition/","title":"Facial Emotion Recognition in the Wild (SFEW)","text":"<p>Open the notebook \u2192 Download the report (PDF) \u2192</p>"},{"location":"projects/emotion-recognition/#summary","title":"Summary","text":"<p>I compared feature-engineered classifiers (SVM) with deep learning approaches (custom CNN and VGG-16 fine-tuning) for Facial Emotion Recognition (FER) on Static Facial Expressions in the Wild (SFEW) dataset. Classic models set a baseline, a small CNN improved results, and fine-tuning with VGG-16 achieved the best performance (44.12% test accuracy). Results reflect the difficulty of FER under real-world conditions (pose, lighting, occlusion) and the limits of hand-crafted features.</p>"},{"location":"projects/emotion-recognition/#data","title":"Data","text":"<ul> <li>SFEW: 674 images from 38 films; 7 classes (angry, disgust, fear, happy, neutral, sad, surprise).  </li> <li>Preprocessing: face detection/cropping (MTCNN + manual fixes), standardisation; on-the-fly augmentation (flips, rotation); colour inputs performed better than greyscale.</li> </ul>"},{"location":"projects/emotion-recognition/#methods","title":"Methods","text":"<ul> <li>SVM (RBF) baseline with grid search (5-fold CV).  </li> <li>CNN: 3-block conv net with batch norm, max-pooling; tuned LR/weight decay/batch size; early stopping.  </li> <li>Transfer learning: VGG-16 pretrained on ImageNet; classifier replaced for 7 classes; staged unfreezing and fine-tuning.</li> </ul>"},{"location":"projects/emotion-recognition/#results","title":"Results","text":"<ul> <li>SVM (5-fold CV): ~23.9% accuracy.  </li> <li>Custom CNN: ~38\u201340% on held-out test (single-run variability).  </li> <li>VGG-16 fine-tuned: 44.12% test accuracy (best).  </li> <li>Class difficulty uneven; disgust/neutral/surprise remained hardest; happy was most reliably detected.</li> </ul>"},{"location":"projects/emotion-recognition/#takeaways","title":"Takeaways","text":"<ul> <li>Transfer learning materially outperforms hand-crafted features and shallow models on SFEW.  </li> <li>Small data + unconstrained conditions limit absolute accuracy; careful augmentation and regularisation help but do not close the gap.</li> </ul>"},{"location":"projects/emotion-recognition/#next-steps","title":"Next steps","text":"<ul> <li>Stronger backbones (e.g. ResNet/ViT), improved augmentation (cutout/mixup), repeated stratified CV for stability.  </li> <li>Error analysis by pose/occlusion; consider facial landmarks or multi-modal signals (audio/physiology).</li> </ul>"},{"location":"projects/emotion-recognition/#visuals","title":"Visuals","text":"<p>--&gt;</p> <p>Full repo: https://github.com/ClaireLouiseR/COMP8420</p>"},{"location":"projects/emotion-recognition/emotion_recognition/","title":"Emotion recognition","text":"In\u00a0[\u00a0]: Copied! <pre>import torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport numpy as np\nfrom torchvision import models\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import Subset\nimport copy\n</pre> import torch.nn as nn import torch.optim as optim import os from PIL import Image import matplotlib.pyplot as plt import torch from torchvision import datasets, transforms from torch.utils.data import DataLoader import torch.nn.functional as F from torch.optim.lr_scheduler import ReduceLROnPlateau from sklearn.metrics import confusion_matrix import seaborn as sns import numpy as np from torchvision import models from sklearn.model_selection import KFold from torch.utils.data import Subset import copy  In\u00a0[\u00a0]: Copied! <pre>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nUSE_RESNET = False\n</pre> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(device)  USE_RESNET = False <pre>cuda\n</pre> In\u00a0[\u00a0]: Copied! <pre># Paths\nROOT_DIR = \"/content/drive/MyDrive/faces-emotion_v2\"\nTRAIN_PATH = os.path.join(ROOT_DIR, \"train_split\")\nVAL_PATH = os.path.join(ROOT_DIR, \"val_split\")\nTEST_PATH = os.path.join(ROOT_DIR, \"test_split\")\n\ndef compute_mean_std(loader):\n    \"\"\"\n    Compute the mean and standard deviation of the dataset.\n    \"\"\"\n    # Initialise lists to store per-channel sum, sum of squares, and number of pixels\n    channel_sum = torch.zeros(3)\n    channel_sum_squared = torch.zeros(3)\n    num_pixels = 0\n\n    for images, _ in loader:\n        # Update per-channel sum and sum of squares\n        channel_sum += torch.sum(images, dim=[0, 2, 3])\n        channel_sum_squared += torch.sum(images**2, dim=[0, 2, 3])\n        num_pixels += images.numel() // 3  # Number of pixels in the batch\n\n    # Calculate mean and standard deviation\n    mean = channel_sum / num_pixels\n    std = (channel_sum_squared / num_pixels - mean**2)**0.5\n\n    return mean, std\n\ndef get_transforms(mean, std):\n    transform_train = transforms.Compose([\n        # Resize\n        transforms.Resize((256, 256)),\n\n        # Random horizontal flip\n        transforms.RandomHorizontalFlip(),\n\n        # Random rotation\n        transforms.RandomRotation(10),\n\n        # Brightness, Contrast, and Saturation adjustments\n        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n\n        # Gaussian Blur\n        #transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n\n        # Convert to tensor\n        transforms.ToTensor(),\n\n        # Normalise\n        transforms.Normalize(mean, std)\n    ])\n\n    transform_val_test = transforms.Compose([\n        # Resize\n        transforms.Resize((256, 256)),\n\n        # Convert to tensor\n        transforms.ToTensor(),\n\n        # Normalise\n        transforms.Normalize(mean, std)\n    ])\n\n    return transform_train, transform_val_test\n\ndef create_data_loaders():\n\n    # Create an initial loader to compute mean and std without any normalisation\n    initial_transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n    ])\n    initial_dataset = datasets.ImageFolder(TRAIN_PATH, transform=initial_transform)\n    initial_loader = DataLoader(initial_dataset, batch_size=16, shuffle=False, num_workers=2)\n\n    # Compute mean and standard deviation\n    mean, std = compute_mean_std(initial_loader)\n\n    # Get the appropriate transforms\n    transform_train, transform_val_test = get_transforms(mean, std)\n\n    # Create the datasets with the computed normalisation\n    train_dataset = datasets.ImageFolder(TRAIN_PATH, transform=transform_train)\n    val_dataset = datasets.ImageFolder(VAL_PATH, transform=transform_val_test)\n    test_dataset = datasets.ImageFolder(TEST_PATH, transform=transform_val_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n</pre> # Paths ROOT_DIR = \"/content/drive/MyDrive/faces-emotion_v2\" TRAIN_PATH = os.path.join(ROOT_DIR, \"train_split\") VAL_PATH = os.path.join(ROOT_DIR, \"val_split\") TEST_PATH = os.path.join(ROOT_DIR, \"test_split\")  def compute_mean_std(loader):     \"\"\"     Compute the mean and standard deviation of the dataset.     \"\"\"     # Initialise lists to store per-channel sum, sum of squares, and number of pixels     channel_sum = torch.zeros(3)     channel_sum_squared = torch.zeros(3)     num_pixels = 0      for images, _ in loader:         # Update per-channel sum and sum of squares         channel_sum += torch.sum(images, dim=[0, 2, 3])         channel_sum_squared += torch.sum(images**2, dim=[0, 2, 3])         num_pixels += images.numel() // 3  # Number of pixels in the batch      # Calculate mean and standard deviation     mean = channel_sum / num_pixels     std = (channel_sum_squared / num_pixels - mean**2)**0.5      return mean, std  def get_transforms(mean, std):     transform_train = transforms.Compose([         # Resize         transforms.Resize((256, 256)),          # Random horizontal flip         transforms.RandomHorizontalFlip(),          # Random rotation         transforms.RandomRotation(10),          # Brightness, Contrast, and Saturation adjustments         #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),          # Gaussian Blur         #transforms.GaussianBlur(3, sigma=(0.1, 2.0)),          # Convert to tensor         transforms.ToTensor(),          # Normalise         transforms.Normalize(mean, std)     ])      transform_val_test = transforms.Compose([         # Resize         transforms.Resize((256, 256)),          # Convert to tensor         transforms.ToTensor(),          # Normalise         transforms.Normalize(mean, std)     ])      return transform_train, transform_val_test  def create_data_loaders():      # Create an initial loader to compute mean and std without any normalisation     initial_transform = transforms.Compose([         transforms.Resize((256, 256)),         transforms.ToTensor(),     ])     initial_dataset = datasets.ImageFolder(TRAIN_PATH, transform=initial_transform)     initial_loader = DataLoader(initial_dataset, batch_size=16, shuffle=False, num_workers=2)      # Compute mean and standard deviation     mean, std = compute_mean_std(initial_loader)      # Get the appropriate transforms     transform_train, transform_val_test = get_transforms(mean, std)      # Create the datasets with the computed normalisation     train_dataset = datasets.ImageFolder(TRAIN_PATH, transform=transform_train)     val_dataset = datasets.ImageFolder(VAL_PATH, transform=transform_val_test)     test_dataset = datasets.ImageFolder(TEST_PATH, transform=transform_val_test)      train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)     val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)     test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)      return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset In\u00a0[\u00a0]: Copied! <pre>class EmotionCNN(nn.Module):\n    def __init__(self, num_classes=7):\n        super(EmotionCNN, self).__init__()\n\n        # Convolutional Layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n\n        # MaxPooling Layers\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Fully Connected Layers\n        self.fc1 = nn.Linear(128 * 32 * 32, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n\n        # Batch Normalisation\n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n\n    def forward(self, x):\n        # Layer 1\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n\n        # Layer 2\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n\n        # Layer 3\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n\n        # Flattening the layer\n        x = x.view(-1, 128 * 32 * 32)\n\n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return x\n\n\n\n# Instantiate the model\nmodel = EmotionCNN().to(device)  # Move model to GPU\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss().to(device)\nweight_decay = 1e-4\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=weight_decay)\n</pre> class EmotionCNN(nn.Module):     def __init__(self, num_classes=7):         super(EmotionCNN, self).__init__()          # Convolutional Layers         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)          # MaxPooling Layers         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)          # Fully Connected Layers         self.fc1 = nn.Linear(128 * 32 * 32, 256)         self.fc2 = nn.Linear(256, 128)         self.fc3 = nn.Linear(128, num_classes)          # Batch Normalisation         self.bn1 = nn.BatchNorm2d(32)         self.bn2 = nn.BatchNorm2d(64)         self.bn3 = nn.BatchNorm2d(128)      def forward(self, x):         # Layer 1         x = self.pool(F.relu(self.bn1(self.conv1(x))))          # Layer 2         x = self.pool(F.relu(self.bn2(self.conv2(x))))          # Layer 3         x = self.pool(F.relu(self.bn3(self.conv3(x))))          # Flattening the layer         x = x.view(-1, 128 * 32 * 32)          # Fully connected layers         x = F.relu(self.fc1(x))         x = F.relu(self.fc2(x))         x = self.fc3(x)          return x    # Instantiate the model model = EmotionCNN().to(device)  # Move model to GPU  # Define the loss function and optimizer criterion = nn.CrossEntropyLoss().to(device) weight_decay = 1e-4 optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=weight_decay)  In\u00a0[\u00a0]: Copied! <pre>def train(model, train_loader, criterion, optimizer):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    correct = 0\n\n    for images, labels in train_loader:\n        # Move data to GPU if available\n        images, labels = images.to(device), labels.to(device)\n\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Track training loss and accuracy\n        total_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n\n    average_loss = total_loss / len(train_loader.dataset)\n    accuracy = 100. * correct / len(train_loader.dataset)\n    return average_loss, accuracy\n\n\ndef validate(model, val_loader, criterion):\n    model.eval()  # Set the model to evaluation mode\n    total_loss = 0.0\n    correct = 0\n\n    with torch.no_grad():  # No gradient computation during evaluation\n        for images, labels in val_loader:\n            # Move data to GPU if available\n            images, labels = images.cuda(), labels.cuda()\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Track validation loss and accuracy\n            total_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n\n    average_loss = total_loss / len(val_loader.dataset)\n    accuracy = 100. * correct / len(val_loader.dataset)\n    return average_loss, accuracy\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    matrix = confusion_matrix(y_true, y_pred)\n    # Normalise by row (i.e., by true class)\n    matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n    matrix_percentage = matrix_normalized * 100  # Convert to percentage\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(matrix_percentage, annot=True, fmt=\".2f\", cmap='Greens', ax=ax, cbar=False,\n                xticklabels=classes, yticklabels=classes)\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    plt.show()\n\ndef evaluate_on_test(model, test_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n\n    # Plot confusion matrix\n    class_names = test_loader.dataset.classes\n    plot_confusion_matrix(all_labels, all_preds, class_names)\n</pre> def train(model, train_loader, criterion, optimizer):     model.train()  # Set the model to training mode     total_loss = 0.0     correct = 0      for images, labels in train_loader:         # Move data to GPU if available         images, labels = images.to(device), labels.to(device)           # Zero the parameter gradients         optimizer.zero_grad()          # Forward pass         outputs = model(images)         loss = criterion(outputs, labels)          # Backward pass and optimization         loss.backward()         optimizer.step()          # Track training loss and accuracy         total_loss += loss.item() * images.size(0)         _, predicted = outputs.max(1)         correct += predicted.eq(labels).sum().item()      average_loss = total_loss / len(train_loader.dataset)     accuracy = 100. * correct / len(train_loader.dataset)     return average_loss, accuracy   def validate(model, val_loader, criterion):     model.eval()  # Set the model to evaluation mode     total_loss = 0.0     correct = 0      with torch.no_grad():  # No gradient computation during evaluation         for images, labels in val_loader:             # Move data to GPU if available             images, labels = images.cuda(), labels.cuda()              # Forward pass             outputs = model(images)             loss = criterion(outputs, labels)              # Track validation loss and accuracy             total_loss += loss.item() * images.size(0)             _, predicted = outputs.max(1)             correct += predicted.eq(labels).sum().item()      average_loss = total_loss / len(val_loader.dataset)     accuracy = 100. * correct / len(val_loader.dataset)     return average_loss, accuracy  def plot_confusion_matrix(y_true, y_pred, classes):     matrix = confusion_matrix(y_true, y_pred)     # Normalise by row (i.e., by true class)     matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]     matrix_percentage = matrix_normalized * 100  # Convert to percentage     fig, ax = plt.subplots(figsize=(10, 8))     sns.heatmap(matrix_percentage, annot=True, fmt=\".2f\", cmap='Greens', ax=ax, cbar=False,                 xticklabels=classes, yticklabels=classes)     ax.set_xlabel('Predicted labels')     ax.set_ylabel('True labels')     plt.show()  def evaluate_on_test(model, test_loader):     model.eval()     all_preds = []     all_labels = []      with torch.no_grad():         for images, labels in test_loader:             images, labels = images.to(device), labels.to(device)             outputs = model(images)             _, preds = torch.max(outputs, 1)             all_preds.extend(preds.cpu().numpy())             all_labels.extend(labels.cpu().numpy())      accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)     print(f\"Test Accuracy: {accuracy*100:.2f}%\")      # Plot confusion matrix     class_names = test_loader.dataset.classes     plot_confusion_matrix(all_labels, all_preds, class_names)  In\u00a0[\u00a0]: Copied! <pre>def show_transformed_images(dataset, images_per_class=10):\n    class_counts = {}\n    images_to_show = []\n    labels_to_show = []\n\n    for img, label in dataset:\n        # Update count for the current class\n        class_counts[label] = class_counts.get(label, 0) + 1\n\n        # Only process if we haven't seen the class 10 times yet\n        if class_counts[label] &lt;= images_per_class:\n            # Inverse normalise the image for display\n            img = img / 2 + 0.5\n            img = img.clamp(0, 1)  # Ensure values are in [0, 1]\n            images_to_show.append(img)\n            labels_to_show.append(label)\n\n        # Stop if we've seen each class 10 times\n        if len(class_counts) == len(dataset.classes) and all(count == images_per_class for count in class_counts.values()):\n            break\n\n    # Calculate number of rows and columns for display\n    num_rows = len(dataset.classes)\n    num_cols = images_per_class\n\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n    for i, ax_row in enumerate(axs):\n        for j, ax in enumerate(ax_row):\n            idx = i * images_per_class + j\n            img_np = images_to_show[idx].numpy().transpose((1, 2, 0))\n            ax.imshow(img_np)\n            # Set title for the first image in each row\n            if j == 0:\n                ax.set_title(dataset.classes[labels_to_show[idx]])\n            ax.axis('off')\n\n    # Adjust the vertical space between rows\n    plt.subplots_adjust(hspace=0.0)\n    plt.tight_layout()\n    plt.show()\n</pre> def show_transformed_images(dataset, images_per_class=10):     class_counts = {}     images_to_show = []     labels_to_show = []      for img, label in dataset:         # Update count for the current class         class_counts[label] = class_counts.get(label, 0) + 1          # Only process if we haven't seen the class 10 times yet         if class_counts[label] &lt;= images_per_class:             # Inverse normalise the image for display             img = img / 2 + 0.5             img = img.clamp(0, 1)  # Ensure values are in [0, 1]             images_to_show.append(img)             labels_to_show.append(label)          # Stop if we've seen each class 10 times         if len(class_counts) == len(dataset.classes) and all(count == images_per_class for count in class_counts.values()):             break      # Calculate number of rows and columns for display     num_rows = len(dataset.classes)     num_cols = images_per_class      fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))     for i, ax_row in enumerate(axs):         for j, ax in enumerate(ax_row):             idx = i * images_per_class + j             img_np = images_to_show[idx].numpy().transpose((1, 2, 0))             ax.imshow(img_np)             # Set title for the first image in each row             if j == 0:                 ax.set_title(dataset.classes[labels_to_show[idx]])             ax.axis('off')      # Adjust the vertical space between rows     plt.subplots_adjust(hspace=0.0)     plt.tight_layout()     plt.show()  In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders()\n    show_transformed_images(train_dataset)\n    if USE_RESNET:\n        # Load a pre-trained ResNet and modify the last layer\n        model = models.resnet50(pretrained=True)\n        num_features = model.fc.in_features\n        model.fc = nn.Linear(num_features, 7)  # 7 classes\n    else:\n        model = EmotionCNN()\n\n    model = model.cuda()\n    criterion = nn.CrossEntropyLoss().cuda()\n    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n    num_epochs = 50\n\n    for epoch in range(num_epochs):\n        train_loss, train_accuracy = train(model, train_loader, criterion, optimizer)\n        val_loss, val_accuracy = validate(model, val_loader, criterion)\n\n        # Update learning rate based on validation loss\n        # scheduler.step(val_loss)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n        print(\"-\" * 30)\n\n    evaluate_on_test(model, test_loader)\n</pre>  if __name__ == \"__main__\":     train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders()     show_transformed_images(train_dataset)     if USE_RESNET:         # Load a pre-trained ResNet and modify the last layer         model = models.resnet50(pretrained=True)         num_features = model.fc.in_features         model.fc = nn.Linear(num_features, 7)  # 7 classes     else:         model = EmotionCNN()      model = model.cuda()     criterion = nn.CrossEntropyLoss().cuda()     optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)     num_epochs = 50      for epoch in range(num_epochs):         train_loss, train_accuracy = train(model, train_loader, criterion, optimizer)         val_loss, val_accuracy = validate(model, val_loader, criterion)          # Update learning rate based on validation loss         # scheduler.step(val_loss)          print(f\"Epoch {epoch + 1}/{num_epochs}\")         print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")         print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")         print(\"-\" * 30)      evaluate_on_test(model, test_loader)      In\u00a0[\u00a0]: Copied! <pre># Fine tuning\nif __name__ == \"__main__\":\n    train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders()\n\n    # Load a pre-trained VGG-16 model\n    model = models.vgg16(pretrained=True)\n\n    # Freeze all layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Modify the last fully connected layer to match the number of classes in the FER dataset\n    num_classes = 7\n    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n\n    # Unfreeze the fully connected layers (classifier part) for fine-tuning\n    for param in model.classifier.parameters():\n        param.requires_grad = True\n\n    model = model.cuda()\n    criterion = nn.CrossEntropyLoss().cuda()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=1e-4)\n    num_epochs = 40\n\n    for epoch in range(num_epochs):\n        train_loss, train_accuracy = train(model, train_loader, criterion, optimizer)\n        val_loss, val_accuracy = validate(model, val_loader, criterion)\n\n        # Update learning rate based on validation loss\n        # scheduler.step(val_loss)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n        print(\"-\" * 30)\n\n    evaluate_on_test(model, test_loader)\n</pre> # Fine tuning if __name__ == \"__main__\":     train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = create_data_loaders()      # Load a pre-trained VGG-16 model     model = models.vgg16(pretrained=True)      # Freeze all layers     for param in model.parameters():         param.requires_grad = False      # Modify the last fully connected layer to match the number of classes in the FER dataset     num_classes = 7     model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)      # Unfreeze the fully connected layers (classifier part) for fine-tuning     for param in model.classifier.parameters():         param.requires_grad = True      model = model.cuda()     criterion = nn.CrossEntropyLoss().cuda()     optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=1e-4)     num_epochs = 40      for epoch in range(num_epochs):         train_loss, train_accuracy = train(model, train_loader, criterion, optimizer)         val_loss, val_accuracy = validate(model, val_loader, criterion)          # Update learning rate based on validation loss         # scheduler.step(val_loss)          print(f\"Epoch {epoch + 1}/{num_epochs}\")         print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")         print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")         print(\"-\" * 30)      evaluate_on_test(model, test_loader)  <pre>Exception ignored in: &lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7994423870a0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n    if w.is_alive():\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: Exception ignored in: can only test a child process\n&lt;function _MultiProcessingDataLoaderIter.__del__ at 0x7994423870a0&gt;\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n    if w.is_alive():\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\nAssertionError: can only test a child process\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 528M/528M [00:10&lt;00:00, 54.0MB/s]\n</pre> <pre>Epoch 1/40\nTrain Loss: 1.9821, Train Accuracy: 15.68%\nValidation Loss: 1.9288, Validation Accuracy: 11.94%\n------------------------------\nEpoch 2/40\nTrain Loss: 1.9265, Train Accuracy: 18.86%\nValidation Loss: 1.9050, Validation Accuracy: 20.90%\n------------------------------\nEpoch 3/40\nTrain Loss: 1.8578, Train Accuracy: 25.21%\nValidation Loss: 1.8902, Validation Accuracy: 23.88%\n------------------------------\nEpoch 4/40\nTrain Loss: 1.8186, Train Accuracy: 29.87%\nValidation Loss: 1.8744, Validation Accuracy: 23.88%\n------------------------------\nEpoch 5/40\nTrain Loss: 1.7928, Train Accuracy: 31.36%\nValidation Loss: 1.8575, Validation Accuracy: 28.36%\n------------------------------\nEpoch 6/40\nTrain Loss: 1.7530, Train Accuracy: 33.47%\nValidation Loss: 1.8391, Validation Accuracy: 26.87%\n------------------------------\nEpoch 7/40\nTrain Loss: 1.7191, Train Accuracy: 36.44%\nValidation Loss: 1.8252, Validation Accuracy: 29.85%\n------------------------------\nEpoch 8/40\nTrain Loss: 1.6374, Train Accuracy: 41.95%\nValidation Loss: 1.8100, Validation Accuracy: 29.85%\n------------------------------\nEpoch 9/40\nTrain Loss: 1.6008, Train Accuracy: 42.16%\nValidation Loss: 1.7957, Validation Accuracy: 29.85%\n------------------------------\nEpoch 10/40\nTrain Loss: 1.5623, Train Accuracy: 45.34%\nValidation Loss: 1.7779, Validation Accuracy: 31.34%\n------------------------------\nEpoch 11/40\nTrain Loss: 1.4724, Train Accuracy: 48.31%\nValidation Loss: 1.7714, Validation Accuracy: 29.85%\n------------------------------\nEpoch 12/40\nTrain Loss: 1.4484, Train Accuracy: 49.15%\nValidation Loss: 1.7598, Validation Accuracy: 31.34%\n------------------------------\nEpoch 13/40\nTrain Loss: 1.4281, Train Accuracy: 50.85%\nValidation Loss: 1.7458, Validation Accuracy: 34.33%\n------------------------------\nEpoch 14/40\nTrain Loss: 1.3563, Train Accuracy: 52.97%\nValidation Loss: 1.7316, Validation Accuracy: 34.33%\n------------------------------\nEpoch 15/40\nTrain Loss: 1.3303, Train Accuracy: 54.87%\nValidation Loss: 1.7214, Validation Accuracy: 34.33%\n------------------------------\nEpoch 16/40\nTrain Loss: 1.2428, Train Accuracy: 58.47%\nValidation Loss: 1.7178, Validation Accuracy: 32.84%\n------------------------------\nEpoch 17/40\nTrain Loss: 1.1838, Train Accuracy: 60.17%\nValidation Loss: 1.7151, Validation Accuracy: 31.34%\n------------------------------\nEpoch 18/40\nTrain Loss: 1.1443, Train Accuracy: 61.44%\nValidation Loss: 1.7242, Validation Accuracy: 32.84%\n------------------------------\nEpoch 19/40\nTrain Loss: 1.1062, Train Accuracy: 63.98%\nValidation Loss: 1.7138, Validation Accuracy: 34.33%\n------------------------------\nEpoch 20/40\nTrain Loss: 1.1219, Train Accuracy: 60.81%\nValidation Loss: 1.6961, Validation Accuracy: 35.82%\n------------------------------\nEpoch 21/40\nTrain Loss: 1.0716, Train Accuracy: 63.14%\nValidation Loss: 1.7019, Validation Accuracy: 32.84%\n------------------------------\nEpoch 22/40\nTrain Loss: 1.0309, Train Accuracy: 65.68%\nValidation Loss: 1.6949, Validation Accuracy: 34.33%\n------------------------------\nEpoch 23/40\nTrain Loss: 0.9921, Train Accuracy: 69.70%\nValidation Loss: 1.7074, Validation Accuracy: 34.33%\n------------------------------\nEpoch 24/40\nTrain Loss: 0.9710, Train Accuracy: 70.13%\nValidation Loss: 1.7037, Validation Accuracy: 37.31%\n------------------------------\nEpoch 25/40\nTrain Loss: 0.9014, Train Accuracy: 71.40%\nValidation Loss: 1.7107, Validation Accuracy: 37.31%\n------------------------------\nEpoch 26/40\nTrain Loss: 0.8798, Train Accuracy: 70.76%\nValidation Loss: 1.7184, Validation Accuracy: 35.82%\n------------------------------\nEpoch 27/40\nTrain Loss: 0.8285, Train Accuracy: 73.73%\nValidation Loss: 1.7390, Validation Accuracy: 34.33%\n------------------------------\nEpoch 28/40\nTrain Loss: 0.8236, Train Accuracy: 75.00%\nValidation Loss: 1.7176, Validation Accuracy: 37.31%\n------------------------------\nEpoch 29/40\nTrain Loss: 0.7567, Train Accuracy: 78.60%\nValidation Loss: 1.7133, Validation Accuracy: 37.31%\n------------------------------\nEpoch 30/40\nTrain Loss: 0.7409, Train Accuracy: 78.18%\nValidation Loss: 1.7376, Validation Accuracy: 35.82%\n------------------------------\nEpoch 31/40\nTrain Loss: 0.7102, Train Accuracy: 79.45%\nValidation Loss: 1.7220, Validation Accuracy: 37.31%\n------------------------------\nEpoch 32/40\nTrain Loss: 0.7137, Train Accuracy: 77.75%\nValidation Loss: 1.7358, Validation Accuracy: 34.33%\n------------------------------\nEpoch 33/40\nTrain Loss: 0.6700, Train Accuracy: 79.24%\nValidation Loss: 1.7508, Validation Accuracy: 38.81%\n------------------------------\nEpoch 34/40\nTrain Loss: 0.6397, Train Accuracy: 80.08%\nValidation Loss: 1.7697, Validation Accuracy: 38.81%\n------------------------------\nEpoch 35/40\nTrain Loss: 0.6209, Train Accuracy: 82.84%\nValidation Loss: 1.7584, Validation Accuracy: 37.31%\n------------------------------\nEpoch 36/40\nTrain Loss: 0.5879, Train Accuracy: 82.84%\nValidation Loss: 1.7573, Validation Accuracy: 37.31%\n------------------------------\nEpoch 37/40\nTrain Loss: 0.5462, Train Accuracy: 84.11%\nValidation Loss: 1.7786, Validation Accuracy: 38.81%\n------------------------------\nEpoch 38/40\nTrain Loss: 0.5358, Train Accuracy: 84.53%\nValidation Loss: 1.7888, Validation Accuracy: 37.31%\n------------------------------\nEpoch 39/40\nTrain Loss: 0.5002, Train Accuracy: 85.81%\nValidation Loss: 1.8196, Validation Accuracy: 35.82%\n------------------------------\nEpoch 40/40\nTrain Loss: 0.4780, Train Accuracy: 86.44%\nValidation Loss: 1.7838, Validation Accuracy: 40.30%\n------------------------------\nTest Accuracy: 44.12%\n</pre>"},{"location":"projects/networks/","title":"Leveraging Network Science to Enhance Australia\u2019s Indo-Pacific Diplomacy","text":"<p>Open the notebook \u2192 Download the report (PDF) \u2192</p>"},{"location":"projects/networks/#summary","title":"Summary","text":"<p>I analysed diplomatic and influence networks to identify where Australia can most effectively deepen ties and broker regional influence across the Indo-Pacific.</p>"},{"location":"projects/networks/#data","title":"Data","text":"<ul> <li>Lowy Global Diplomacy Index (2016\u20132023): counts of overseas posts across 66 countries to build a weighted, directed diplomacy graph.  </li> <li>Lowy Asia Power Index: three \u201cnetwork power\u201d graphs from API attributes \u2014 online search interest (OSI), foreign capital investment (FCI), and diplomatic dialogues (DD).</li> </ul>"},{"location":"projects/networks/#methods","title":"Methods","text":"<ul> <li>Weighted, directed graphs (plus one undirected graph for diplomatic dialogues)  </li> <li>Centralities: betweenness, PageRank, HITS (hubs/authorities) </li> <li>Weighted in/out degree, clustering coefficient </li> <li>Louvain communities; compared structures with Adjusted Rand Index (ARI)</li> </ul>"},{"location":"projects/networks/#selected-results","title":"Selected results","text":"<ul> <li> <p>Diplomatic footprint: Australia has expanded posts in Oceania, but remains mid-pack for total posts relative to peers with similar GDP. Diversifying posts beyond Oceania would broaden reach.</p> </li> <li> <p>Brokerage via diplomatic dialogues (DD): High PageRank (and brokerage metrics) indicate Australia is well placed to convene and broker across Indo-Pacific diplomatic fora. Deepen ties with Indonesia, Japan, India while balancing US alignment.</p> </li> <li> <p>Attention flows (OSI): Australia is a net receiver of attention (\u2248 6th by weighted in-degree). Cultural exchange and business initiatives with India and Japan are likely to lift visibility.</p> </li> <li> <p>Investment flows (FCI): China dominates as an investment hub; Australia\u2019s hub score is lower. Emphasise quality over volume in sectors of advantage (e.g., critical minerals, renewables, education), leveraging governance and reliability.</p> </li> <li> <p>Community structure: Stable regional ties to New Zealand; economic links cluster with Southeast Asia. OSI and DD communities overlap moderately; FCI forms distinct blocs \u2192 different policy levers by network.</p> </li> <li> <p>Actionable next steps: Target brokerage opportunities in triads (e.g., Japan\u2013India\u2013Australia, ASEAN pairings), strengthen weak ties identified by low edge weights, and track changes 2016\u20132023 to sequence postings and dialogues.</p> </li> </ul>"},{"location":"projects/networks/#selected-visuals","title":"Selected Visuals","text":"<p>Full repo: https://github.com/ClaireLouiseR/COMP8880</p>"},{"location":"projects/networks/analysis/","title":"Notebook","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\nimport sklearn\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\nimport networkx.algorithms.community as nx_comm\nfrom matplotlib.ticker import MaxNLocator\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.cm as cm\nfrom sklearn.metrics import adjusted_rand_score\nfrom node2vec import Node2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\n</pre> import pandas as pd import networkx as nx import matplotlib.pyplot as plt import json import numpy as np import sklearn import matplotlib.patches as mpatches import matplotlib.lines as mlines import networkx.algorithms.community as nx_comm from matplotlib.ticker import MaxNLocator from sklearn.metrics import silhouette_score import matplotlib.cm as cm from sklearn.metrics import adjusted_rand_score from node2vec import Node2Vec from sklearn.metrics.pairwise import cosine_similarity  In\u00a0[2]: Copied! <pre># Load the CSV files into Python\nnetwork_power_df = pd.read_csv('network_power.csv')\npost_counts_df = pd.read_csv('post_counts.csv')\ncountries_df = pd.read_csv('countries.csv')\ncountry_attributes_df = pd.read_csv('country_attributes.csv')\n</pre> # Load the CSV files into Python network_power_df = pd.read_csv('network_power.csv') post_counts_df = pd.read_csv('post_counts.csv') countries_df = pd.read_csv('countries.csv') country_attributes_df = pd.read_csv('country_attributes.csv') In\u00a0[3]: Copied! <pre>## POST COUNTS\n\ndef create_graphs(post_counts_df, countries_df, country_attributes_df):\n    # Create a dictionary to store the graphs by year\n    graphs_by_year = {}\n    \n    # Merge post_counts_df with countries_df to get subregion and subregionColour\n    post_counts_df = post_counts_df.merge(countries_df[['id', 'subregion', 'subregionColour']], \n                                          left_on='COUNTRY', right_on='id', how='left')\n    post_counts_df = post_counts_df.merge(countries_df[['id', 'subregion', 'subregionColour']], \n                                          left_on='POST COUNTRY', right_on='id', how='left', \n                                          suffixes=('', '_POST'))\n    \n    # Split country_attributes_df by year\n    attributes_by_year = country_attributes_df.groupby('Year')\n    \n    for year, attributes_df in attributes_by_year:\n        # Filter post_counts_df for the current year\n        yearly_posts = post_counts_df[post_counts_df['Year'] == year]\n        \n        # Create a directed graph\n        G = nx.DiGraph()\n        \n        # Add nodes with attributes\n        for _, row in countries_df.iterrows():\n            country_id = row['id']\n            G.add_node(country_id, subregion=row['subregion'], subregionColour=row['subregionColour'])\n        \n        # Update nodes with yearly attributes\n        for _, row in attributes_df.iterrows():\n            country_id = row['COUNTRY']\n            if country_id in G.nodes:\n                G.nodes[country_id].update({\n                    'POPULATION (M)': row['POPULATION (M)'],\n                    'GDP (B, USD)': row['GDP (B, USD)'],\n                    'G20 RANK': row['G20 RANK'],\n                    'OECD RANK': row['OECD RANK'],\n                    'ASIA RANK': row['ASIA RANK'],\n                    'OVERALL RANK': row['OVERALL RANK']\n                })\n        \n        # Add edges with weights\n        for _, row in yearly_posts.iterrows():\n            G.add_edge(row['COUNTRY'], row['POST COUNTRY'], weight=row['POST COUNT'])\n        \n        # Store the graph in the dictionary\n        graphs_by_year[year] = G\n    \n    return graphs_by_year\n\ngraphs = create_graphs(post_counts_df, countries_df, country_attributes_df)\n\n# Accessing individual graphs\nposts_2016 = graphs.get(2016)\nposts_2017 = graphs.get(2017)\nposts_2019 = graphs.get(2019)\nposts_2021 = graphs.get(2021)\nposts_2023 = graphs.get(2023)\n\n# Export function\n# def export_graphs(graphs):\n#     for year, graph in graphs.items():\n#         filename = f\"{year}.gexf\"\n#         nx.write_gexf(graph, filename)\n#         print(f\"Graph for {year} exported to {filename}\")\n\n# export_graphs(graphs_by_year)\n</pre> ## POST COUNTS  def create_graphs(post_counts_df, countries_df, country_attributes_df):     # Create a dictionary to store the graphs by year     graphs_by_year = {}          # Merge post_counts_df with countries_df to get subregion and subregionColour     post_counts_df = post_counts_df.merge(countries_df[['id', 'subregion', 'subregionColour']],                                            left_on='COUNTRY', right_on='id', how='left')     post_counts_df = post_counts_df.merge(countries_df[['id', 'subregion', 'subregionColour']],                                            left_on='POST COUNTRY', right_on='id', how='left',                                            suffixes=('', '_POST'))          # Split country_attributes_df by year     attributes_by_year = country_attributes_df.groupby('Year')          for year, attributes_df in attributes_by_year:         # Filter post_counts_df for the current year         yearly_posts = post_counts_df[post_counts_df['Year'] == year]                  # Create a directed graph         G = nx.DiGraph()                  # Add nodes with attributes         for _, row in countries_df.iterrows():             country_id = row['id']             G.add_node(country_id, subregion=row['subregion'], subregionColour=row['subregionColour'])                  # Update nodes with yearly attributes         for _, row in attributes_df.iterrows():             country_id = row['COUNTRY']             if country_id in G.nodes:                 G.nodes[country_id].update({                     'POPULATION (M)': row['POPULATION (M)'],                     'GDP (B, USD)': row['GDP (B, USD)'],                     'G20 RANK': row['G20 RANK'],                     'OECD RANK': row['OECD RANK'],                     'ASIA RANK': row['ASIA RANK'],                     'OVERALL RANK': row['OVERALL RANK']                 })                  # Add edges with weights         for _, row in yearly_posts.iterrows():             G.add_edge(row['COUNTRY'], row['POST COUNTRY'], weight=row['POST COUNT'])                  # Store the graph in the dictionary         graphs_by_year[year] = G          return graphs_by_year  graphs = create_graphs(post_counts_df, countries_df, country_attributes_df)  # Accessing individual graphs posts_2016 = graphs.get(2016) posts_2017 = graphs.get(2017) posts_2019 = graphs.get(2019) posts_2021 = graphs.get(2021) posts_2023 = graphs.get(2023)  # Export function # def export_graphs(graphs): #     for year, graph in graphs.items(): #         filename = f\"{year}.gexf\" #         nx.write_gexf(graph, filename) #         print(f\"Graph for {year} exported to {filename}\")  # export_graphs(graphs_by_year) In\u00a0[4]: Copied! <pre># Function to get general info about a graph\ndef get_graph_info(graph):\n    \"\"\"\n    Retrieve edges with weights and nodes with attributes from a graph.\n\n    Parameters:\n    graph (nx.Graph): The graph to retrieve information from.\n\n    Returns:\n    dict: A dictionary containing nodes and edges information.\n    \"\"\"\n    edges_with_weights = list(graph.edges(data=True))\n    nodes_with_attributes = list(graph.nodes(data=True))\n\n    return {\n        'edges': edges_with_weights,\n        'nodes': nodes_with_attributes\n    }\n</pre> # Function to get general info about a graph def get_graph_info(graph):     \"\"\"     Retrieve edges with weights and nodes with attributes from a graph.      Parameters:     graph (nx.Graph): The graph to retrieve information from.      Returns:     dict: A dictionary containing nodes and edges information.     \"\"\"     edges_with_weights = list(graph.edges(data=True))     nodes_with_attributes = list(graph.nodes(data=True))      return {         'edges': edges_with_weights,         'nodes': nodes_with_attributes     } In\u00a0[5]: Copied! <pre># Function to invert edge weights\ndef invert_weights(graph):\n    transformed_graph = graph.copy()\n    for u, v, d in transformed_graph.edges(data=True):\n        d['transformed_weight'] = 1 / d['weight'] if d['weight'] != 0 else 0\n    return transformed_graph\n    \n# Function to compute and display betweenness centrality\ndef compute_betweenness_centrality(graph, year):\n    \n    graph = invert_weights(graph)\n    \n    betweenness_centrality = nx.betweenness_centrality(graph, weight='transformed_weight')\n    sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda item: item[1], reverse=True)\n    \n    # Find the position of Australia (AU) and the nodes either side\n    au_index = [i for i, (node, _) in enumerate(sorted_betweenness) if node == 'AU'][0]\n    context_nodes = sorted_betweenness[max(0, au_index):min(len(sorted_betweenness), au_index+1)]\n    \n    top_5_nodes = sorted_betweenness[:5]\n    \n    # Extract subregion and subregionColour\n    top_5_nodes = [(node, centrality, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, centrality in top_5_nodes]\n    context_nodes = [(node, centrality, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, centrality in context_nodes]\n    \n    return {\n        'year': year,\n        'top_5_nodes': top_5_nodes,\n        'context_nodes': context_nodes\n    }\n\n# Add node attributes and compute betweenness centrality for each year\nresults = []\nfor year, graph in graphs.items():\n    results.append(compute_betweenness_centrality(graph, year))\n    \n# Create a dictionary to store centrality data\ncentrality_data = {}\nfor result in results:\n    year = result['year']\n    for node, centrality, subregion, subregionColour in result['top_5_nodes'] + result['context_nodes']:\n        if node not in centrality_data:\n            centrality_data[node] = {'years': [], 'centrality': [], 'subregion': subregion, 'subregionColour': subregionColour}\n        centrality_data[node]['years'].append(year)\n        centrality_data[node]['centrality'].append(centrality)\n\n# Plot the betweenness centrality data\nplt.figure(figsize=(10, 5))\nfor node, data in centrality_data.items():\n    plt.plot(data['years'], data['centrality'], marker='o', label=node if node == 'AU' else \"\", color=data['subregionColour'])\n    plt.text(data['years'][-1], data['centrality'][-1], node, fontsize=9)\n\nplt.xlabel('Year')\nplt.ylabel('Betweenness Centrality')\nplt.title('Betweenness Centrality of Key Nodes in Global Diplomatic Networks (2016-2023)\\n(Inverted weights were used to compute shortest paths)')\nplt.legend(loc='best', fontsize='small', ncol=2)\nplt.grid(True)\n\n# plt.savefig('betweenness_centrality.png', format='png', dpi=300)  # Save as PNG with 300 DPI resolution\n\nplt.show()\n</pre> # Function to invert edge weights def invert_weights(graph):     transformed_graph = graph.copy()     for u, v, d in transformed_graph.edges(data=True):         d['transformed_weight'] = 1 / d['weight'] if d['weight'] != 0 else 0     return transformed_graph      # Function to compute and display betweenness centrality def compute_betweenness_centrality(graph, year):          graph = invert_weights(graph)          betweenness_centrality = nx.betweenness_centrality(graph, weight='transformed_weight')     sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda item: item[1], reverse=True)          # Find the position of Australia (AU) and the nodes either side     au_index = [i for i, (node, _) in enumerate(sorted_betweenness) if node == 'AU'][0]     context_nodes = sorted_betweenness[max(0, au_index):min(len(sorted_betweenness), au_index+1)]          top_5_nodes = sorted_betweenness[:5]          # Extract subregion and subregionColour     top_5_nodes = [(node, centrality, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, centrality in top_5_nodes]     context_nodes = [(node, centrality, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, centrality in context_nodes]          return {         'year': year,         'top_5_nodes': top_5_nodes,         'context_nodes': context_nodes     }  # Add node attributes and compute betweenness centrality for each year results = [] for year, graph in graphs.items():     results.append(compute_betweenness_centrality(graph, year))      # Create a dictionary to store centrality data centrality_data = {} for result in results:     year = result['year']     for node, centrality, subregion, subregionColour in result['top_5_nodes'] + result['context_nodes']:         if node not in centrality_data:             centrality_data[node] = {'years': [], 'centrality': [], 'subregion': subregion, 'subregionColour': subregionColour}         centrality_data[node]['years'].append(year)         centrality_data[node]['centrality'].append(centrality)  # Plot the betweenness centrality data plt.figure(figsize=(10, 5)) for node, data in centrality_data.items():     plt.plot(data['years'], data['centrality'], marker='o', label=node if node == 'AU' else \"\", color=data['subregionColour'])     plt.text(data['years'][-1], data['centrality'][-1], node, fontsize=9)  plt.xlabel('Year') plt.ylabel('Betweenness Centrality') plt.title('Betweenness Centrality of Key Nodes in Global Diplomatic Networks (2016-2023)\\n(Inverted weights were used to compute shortest paths)') plt.legend(loc='best', fontsize='small', ncol=2) plt.grid(True)  # plt.savefig('betweenness_centrality.png', format='png', dpi=300)  # Save as PNG with 300 DPI resolution  plt.show() In\u00a0[6]: Copied! <pre># Group by 'COUNTRY' and sum only the 'POST COUNT' column\npost_counts_2023_grouped = post_counts_df[post_counts_df['Year'] == 2023].groupby('COUNTRY')['POST COUNT'].sum().reset_index()\n\n# Filter country attributes for the year 2023\ncountry_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023]\n\n# Merge with post_counts_2023_grouped to get POST COUNT, GDP (B, USD), and POPULATION (M)\nmerged_data = pd.merge(country_attributes_2023[['COUNTRY', 'GDP (B, USD)', 'POPULATION (M)']], post_counts_2023_grouped, on='COUNTRY', how='inner')\n\n# Merge with countries to get subregion and subregionColour\nfinal_data = pd.merge(merged_data, countries_df[['id', 'subregion', 'subregionColour']], left_on='COUNTRY', right_on='id', how='left')\n\n# Filter out the rows where subregion is 'European Union'\nfinal_data_filtered = final_data[final_data['subregion'] != 'European Union']\n\n# Plot the data with log(GDP) on the x-axis and POST COUNT on the y-axis\nplt.figure(figsize=(13, 7))\n\n# Scatter plot with subregion colors\nfor subregion, data in final_data_filtered.groupby('subregion'):\n    plt.scatter(data['GDP (B, USD)'].apply(lambda x: np.log(x)), data['POST COUNT'], \n                color=data['subregionColour'].values[0], label=subregion, s=100)\n\n# Add labels for each point\nfor i, row in final_data_filtered.iterrows():\n    plt.text(np.log(row['GDP (B, USD)']), row['POST COUNT'], row['COUNTRY'], fontsize=10, ha='right')\n\n# Set the labels and title\nplt.xlabel('log(GDP (B, USD))')\nplt.ylabel('Post Count')\nplt.title('log(GDP) vs Post Count for 2023')\n\n# Create a  legend inside the top left of the graph\nplt.legend(loc='upper left', fontsize='x-small', title='Subregion')\n\n# plt.savefig('log(GDP)_vs_Post Count_2023_2.png', format='png', dpi=500)  # Save as PNG with 300 DPI resolution\n\nplt.show()\n</pre> # Group by 'COUNTRY' and sum only the 'POST COUNT' column post_counts_2023_grouped = post_counts_df[post_counts_df['Year'] == 2023].groupby('COUNTRY')['POST COUNT'].sum().reset_index()  # Filter country attributes for the year 2023 country_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023]  # Merge with post_counts_2023_grouped to get POST COUNT, GDP (B, USD), and POPULATION (M) merged_data = pd.merge(country_attributes_2023[['COUNTRY', 'GDP (B, USD)', 'POPULATION (M)']], post_counts_2023_grouped, on='COUNTRY', how='inner')  # Merge with countries to get subregion and subregionColour final_data = pd.merge(merged_data, countries_df[['id', 'subregion', 'subregionColour']], left_on='COUNTRY', right_on='id', how='left')  # Filter out the rows where subregion is 'European Union' final_data_filtered = final_data[final_data['subregion'] != 'European Union']  # Plot the data with log(GDP) on the x-axis and POST COUNT on the y-axis plt.figure(figsize=(13, 7))  # Scatter plot with subregion colors for subregion, data in final_data_filtered.groupby('subregion'):     plt.scatter(data['GDP (B, USD)'].apply(lambda x: np.log(x)), data['POST COUNT'],                  color=data['subregionColour'].values[0], label=subregion, s=100)  # Add labels for each point for i, row in final_data_filtered.iterrows():     plt.text(np.log(row['GDP (B, USD)']), row['POST COUNT'], row['COUNTRY'], fontsize=10, ha='right')  # Set the labels and title plt.xlabel('log(GDP (B, USD))') plt.ylabel('Post Count') plt.title('log(GDP) vs Post Count for 2023')  # Create a  legend inside the top left of the graph plt.legend(loc='upper left', fontsize='x-small', title='Subregion')  # plt.savefig('log(GDP)_vs_Post Count_2023_2.png', format='png', dpi=500)  # Save as PNG with 300 DPI resolution  plt.show() In\u00a0[7]: Copied! <pre># Function to compute and display clustering coefficient\ndef compute_clustering_coefficient_filtered(graph, year):\n    # Identify nodes with outgoing edges\n    outgoing_nodes = {node for node in graph.nodes if graph.out_degree(node) &gt; 0}\n    \n    # Create a subgraph with only nodes that have outgoing edges\n    subgraph = graph.subgraph(outgoing_nodes)\n    \n    clustering_coeffs = nx.clustering(subgraph, weight='weight')\n    sorted_clustering = sorted(clustering_coeffs.items(), key=lambda item: item[1], reverse=True)\n    \n    # Include Australia (AU) in the top nodes if not already present\n    top_5_nodes = sorted_clustering[:5]\n    if 'AU' not in [node for node, _ in top_5_nodes]:\n        for node, clustering in sorted_clustering:\n            if node == 'AU':\n                top_5_nodes.append((node, clustering))\n                break\n    \n    # Extract subregion and subregionColour\n    top_5_nodes = [(node, clustering, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, clustering in top_5_nodes]\n    \n    return {\n        'year': year,\n        'top_5_nodes': top_5_nodes\n    }\n\n# Add node attributes and compute clustering coefficient for each year\nresults = []\nfor year, graph in graphs.items():\n    results.append(compute_clustering_coefficient_filtered(graph, year))\n\n# Create a dictionary to store clustering data\nclustering_data = {}\nfor result in results:\n    year = result['year']\n    for node, clustering, subregion, subregionColour in result['top_5_nodes']:\n        if node not in clustering_data:\n            clustering_data[node] = {'years': [], 'clustering': [], 'subregion': subregion, 'subregionColour': subregionColour}\n        clustering_data[node]['years'].append(year)\n        clustering_data[node]['clustering'].append(clustering)\n\n# Plot the clustering coefficient data\nplt.figure(figsize=(10, 6))\nfor node, data in clustering_data.items():\n    plt.plot(data['years'], data['clustering'], marker='o', label=node, color=data['subregionColour'])\n    plt.text(data['years'][-1], data['clustering'][-1], node, fontsize=9)\n\nplt.xlabel('Year')\nplt.ylabel('Clustering Coefficient')\nplt.title('Clustering Coefficient of Key Nodes in Global Diplomatic Networks (2016-2023)')\nplt.legend(loc='best', fontsize='small', ncol=2)\nplt.grid(True)\nplt.show()\n</pre> # Function to compute and display clustering coefficient def compute_clustering_coefficient_filtered(graph, year):     # Identify nodes with outgoing edges     outgoing_nodes = {node for node in graph.nodes if graph.out_degree(node) &gt; 0}          # Create a subgraph with only nodes that have outgoing edges     subgraph = graph.subgraph(outgoing_nodes)          clustering_coeffs = nx.clustering(subgraph, weight='weight')     sorted_clustering = sorted(clustering_coeffs.items(), key=lambda item: item[1], reverse=True)          # Include Australia (AU) in the top nodes if not already present     top_5_nodes = sorted_clustering[:5]     if 'AU' not in [node for node, _ in top_5_nodes]:         for node, clustering in sorted_clustering:             if node == 'AU':                 top_5_nodes.append((node, clustering))                 break          # Extract subregion and subregionColour     top_5_nodes = [(node, clustering, graph.nodes[node]['subregion'], graph.nodes[node]['subregionColour']) for node, clustering in top_5_nodes]          return {         'year': year,         'top_5_nodes': top_5_nodes     }  # Add node attributes and compute clustering coefficient for each year results = [] for year, graph in graphs.items():     results.append(compute_clustering_coefficient_filtered(graph, year))  # Create a dictionary to store clustering data clustering_data = {} for result in results:     year = result['year']     for node, clustering, subregion, subregionColour in result['top_5_nodes']:         if node not in clustering_data:             clustering_data[node] = {'years': [], 'clustering': [], 'subregion': subregion, 'subregionColour': subregionColour}         clustering_data[node]['years'].append(year)         clustering_data[node]['clustering'].append(clustering)  # Plot the clustering coefficient data plt.figure(figsize=(10, 6)) for node, data in clustering_data.items():     plt.plot(data['years'], data['clustering'], marker='o', label=node, color=data['subregionColour'])     plt.text(data['years'][-1], data['clustering'][-1], node, fontsize=9)  plt.xlabel('Year') plt.ylabel('Clustering Coefficient') plt.title('Clustering Coefficient of Key Nodes in Global Diplomatic Networks (2016-2023)') plt.legend(loc='best', fontsize='small', ncol=2) plt.grid(True) plt.show() In\u00a0[8]: Copied! <pre># Declining clustering coefficient probably because of new nodes added to the network \n\n# Initialise dictionaries to store outgoing nodes and new additions for each year\noutgoing_nodes_by_year = {}\nnew_additions_by_year = {}\n\n# Function to get nodes with outgoing edges for a given graph\ndef get_outgoing_nodes(graph):\n    return {node for node in graph.nodes if graph.out_degree(node) &gt; 0}\n\n# Compute outgoing nodes for each year and track new additions\nprevious_outgoing_nodes = set()\nfor year, graph in graphs.items():\n    current_outgoing_nodes = get_outgoing_nodes(graph)\n    outgoing_nodes_by_year[year] = current_outgoing_nodes\n    new_additions_by_year[year] = current_outgoing_nodes - previous_outgoing_nodes\n    previous_outgoing_nodes = current_outgoing_nodes\n\n# Print the outgoing nodes and new additions for each year\nfor year in sorted(outgoing_nodes_by_year.keys()):\n    print(f\"Year: {year}\")\n    # print(f\"Outgoing Nodes: {outgoing_nodes_by_year[year]}\")\n    print(f\"New Additions: {new_additions_by_year[year]}\")\n    print()\n</pre> # Declining clustering coefficient probably because of new nodes added to the network   # Initialise dictionaries to store outgoing nodes and new additions for each year outgoing_nodes_by_year = {} new_additions_by_year = {}  # Function to get nodes with outgoing edges for a given graph def get_outgoing_nodes(graph):     return {node for node in graph.nodes if graph.out_degree(node) &gt; 0}  # Compute outgoing nodes for each year and track new additions previous_outgoing_nodes = set() for year, graph in graphs.items():     current_outgoing_nodes = get_outgoing_nodes(graph)     outgoing_nodes_by_year[year] = current_outgoing_nodes     new_additions_by_year[year] = current_outgoing_nodes - previous_outgoing_nodes     previous_outgoing_nodes = current_outgoing_nodes  # Print the outgoing nodes and new additions for each year for year in sorted(outgoing_nodes_by_year.keys()):     print(f\"Year: {year}\")     # print(f\"Outgoing Nodes: {outgoing_nodes_by_year[year]}\")     print(f\"New Additions: {new_additions_by_year[year]}\")     print() <pre>Year: 2016\nNew Additions: {'DE', 'SVK', 'UK', 'AR', 'IE', 'DK', 'NZ', 'AU', 'RU', 'CZ', 'NO', 'SA', 'SI', 'AT', 'IT', 'IS', 'NL', 'ZA', 'GR', 'ES', 'PT', 'ID', 'SE', 'CN', 'SK', 'IL', 'LU', 'CH', 'FR', 'TR', 'IN', 'MX', 'PL', 'US', 'JP', 'HU', 'EE', 'FI', 'BRA', 'BE', 'CL', 'CA'}\n\nYear: 2017\nNew Additions: {'TW', 'MA', 'BA', 'LA', 'NK', 'LV', 'MY', 'PH', 'SG', 'TH', 'CB', 'SL', 'VN', 'MO', 'BR', 'PK', 'NP', 'BT'}\n\nYear: 2019\nNew Additions: {'LT'}\n\nYear: 2021\nNew Additions: {'PG', 'EU', 'CO', 'CR'}\n\nYear: 2023\nNew Additions: {'TL'}\n\n</pre> In\u00a0[9]: Copied! <pre># Function to get nodes with outgoing edges and their weighted out-degrees for a given graph\ndef get_weighted_out_degrees(graph):\n    weighted_out_degrees = {node: graph.out_degree(node, weight='weight') for node in graph.nodes if graph.out_degree(node) &gt; 0}\n    return weighted_out_degrees\n\n# Initialise a dictionary to store the results\nout_degrees_by_year = {}\n\n# Compute the weighted out-degrees for each year\nfor year, graph in graphs.items():\n    out_degrees_by_year[year] = get_weighted_out_degrees(graph)\n\n# Collect data for plotting\ndata = []\nfor year, out_degrees in out_degrees_by_year.items():\n    for node, out_degree in out_degrees.items():\n        subregion = graph.nodes[node]['subregion']\n        subregion_colour = graph.nodes[node]['subregionColour']\n        data.append([year, node, out_degree, subregion, subregion_colour])\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=['Year', 'Country', 'OutDegree', 'Subregion', 'SubregionColour'])\n\n# Identify top 4 countries by out-degree and countries with similar out-degrees to Australia\ntop_countries = df.groupby('Country')['OutDegree'].sum().nlargest(5).index.tolist()\nau_out_degrees = df[df['Country'] == 'AU'].groupby('Country')['OutDegree'].sum().values[0]\n\n# Find the countries with the most similar out-degrees to Australia\ndf['OutDegreeDiff'] = (df.groupby('Country')['OutDegree'].transform('sum') - au_out_degrees).abs()\nsimilar_countries = df.loc[df['Country'] != 'AU'].groupby('Country')['OutDegreeDiff'].mean().nsmallest(2).index.tolist()\n\n# Combine the top countries, Australia, and similar countries\nselected_countries = top_countries + ['AU'] + similar_countries\n\n# Filter DataFrame for selected countries\ndf = df[df['Country'].isin(selected_countries)]\n\n# Pivot the DataFrame for plotting\ndf_pivot = df.pivot_table(index='Country', columns='Year', values='OutDegree', fill_value=0)\n\n# Plotting the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot each year as a separate bar in the clustered column chart\nbar_width = 0.15\nyears = sorted(df['Year'].unique())\npositions = range(len(df_pivot.index))\nfor i, year in enumerate(years):\n    year_data = df_pivot[year]\n    bar_positions = [p + i * bar_width for p in positions]\n    country_colors = [df[df['Country'] == country]['SubregionColour'].values[0] for country in df_pivot.index]\n    bars = ax.bar(bar_positions, year_data, width=bar_width, label=year, color=country_colors, edgecolor='black')  # Add edgecolor for border\n\n# Set the position of the x ticks\nax.set_xticks([p + (len(years) - 1) * bar_width / 2 for p in positions])\nax.set_xticklabels(df_pivot.index, rotation=90)\n\n# Adding labels and title\nax.set_xlabel('Country')\nax.set_ylabel('Weighted Out Degree')\nax.set_title('Weighted Out Degree of Countries by Year\\nTop 5 nodes + Australia and 2 context nodes shown\\nColumns within a country cluster from left to right show total number of posts in 2016, 2017, 2019, 2021, and 2023 respectively')\n\n# Create a custom legend for subregions\nsubregions = df[['Subregion', 'SubregionColour']].drop_duplicates().sort_values('Subregion')\nhandles = [plt.Line2D([0], [0], marker='o', color=row['SubregionColour'], markersize=10, linestyle='') for _, row in subregions.iterrows()]\nlabels = subregions['Subregion'].tolist()\nax.legend(handles, labels, title='Subregions', loc='center left', bbox_to_anchor=(1, 0.5))\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n</pre> # Function to get nodes with outgoing edges and their weighted out-degrees for a given graph def get_weighted_out_degrees(graph):     weighted_out_degrees = {node: graph.out_degree(node, weight='weight') for node in graph.nodes if graph.out_degree(node) &gt; 0}     return weighted_out_degrees  # Initialise a dictionary to store the results out_degrees_by_year = {}  # Compute the weighted out-degrees for each year for year, graph in graphs.items():     out_degrees_by_year[year] = get_weighted_out_degrees(graph)  # Collect data for plotting data = [] for year, out_degrees in out_degrees_by_year.items():     for node, out_degree in out_degrees.items():         subregion = graph.nodes[node]['subregion']         subregion_colour = graph.nodes[node]['subregionColour']         data.append([year, node, out_degree, subregion, subregion_colour])  # Create a DataFrame df = pd.DataFrame(data, columns=['Year', 'Country', 'OutDegree', 'Subregion', 'SubregionColour'])  # Identify top 4 countries by out-degree and countries with similar out-degrees to Australia top_countries = df.groupby('Country')['OutDegree'].sum().nlargest(5).index.tolist() au_out_degrees = df[df['Country'] == 'AU'].groupby('Country')['OutDegree'].sum().values[0]  # Find the countries with the most similar out-degrees to Australia df['OutDegreeDiff'] = (df.groupby('Country')['OutDegree'].transform('sum') - au_out_degrees).abs() similar_countries = df.loc[df['Country'] != 'AU'].groupby('Country')['OutDegreeDiff'].mean().nsmallest(2).index.tolist()  # Combine the top countries, Australia, and similar countries selected_countries = top_countries + ['AU'] + similar_countries  # Filter DataFrame for selected countries df = df[df['Country'].isin(selected_countries)]  # Pivot the DataFrame for plotting df_pivot = df.pivot_table(index='Country', columns='Year', values='OutDegree', fill_value=0)  # Plotting the data fig, ax = plt.subplots(figsize=(15, 10))  # Plot each year as a separate bar in the clustered column chart bar_width = 0.15 years = sorted(df['Year'].unique()) positions = range(len(df_pivot.index)) for i, year in enumerate(years):     year_data = df_pivot[year]     bar_positions = [p + i * bar_width for p in positions]     country_colors = [df[df['Country'] == country]['SubregionColour'].values[0] for country in df_pivot.index]     bars = ax.bar(bar_positions, year_data, width=bar_width, label=year, color=country_colors, edgecolor='black')  # Add edgecolor for border  # Set the position of the x ticks ax.set_xticks([p + (len(years) - 1) * bar_width / 2 for p in positions]) ax.set_xticklabels(df_pivot.index, rotation=90)  # Adding labels and title ax.set_xlabel('Country') ax.set_ylabel('Weighted Out Degree') ax.set_title('Weighted Out Degree of Countries by Year\\nTop 5 nodes + Australia and 2 context nodes shown\\nColumns within a country cluster from left to right show total number of posts in 2016, 2017, 2019, 2021, and 2023 respectively')  # Create a custom legend for subregions subregions = df[['Subregion', 'SubregionColour']].drop_duplicates().sort_values('Subregion') handles = [plt.Line2D([0], [0], marker='o', color=row['SubregionColour'], markersize=10, linestyle='') for _, row in subregions.iterrows()] labels = subregions['Subregion'].tolist() ax.legend(handles, labels, title='Subregions', loc='center left', bbox_to_anchor=(1, 0.5))  # Display the plot plt.tight_layout() plt.show() In\u00a0[10]: Copied! <pre># Function to get nodes with incoming edges and their weighted in-degrees for a given graph\ndef get_weighted_in_degrees(graph):\n    weighted_in_degrees = {node: graph.in_degree(node, weight='weight') for node in graph.nodes if graph.in_degree(node) &gt; 0}\n    return weighted_in_degrees\n\n# Initialise a dictionary to store the results\nin_degrees_by_year = {}\n\n# Compute the weighted in-degrees for each year\nfor year, graph in graphs.items():\n    in_degrees_by_year[year] = get_weighted_in_degrees(graph)\n\n# Collect data for plotting\ndata = []\nfor year, in_degrees in in_degrees_by_year.items():\n    for node, in_degree in in_degrees.items():\n        subregion = graph.nodes[node]['subregion']\n        subregion_colour = graph.nodes[node]['subregionColour']\n        data.append([year, node, in_degree, subregion, subregion_colour])\n\n# Create a DataFrame\ndf = pd.DataFrame(data, columns=['Year', 'Country', 'InDegree', 'Subregion', 'SubregionColour'])\n\n# Identify top 4 countries by in-degree and countries with similar in-degrees to Australia\ntop_countries = df.groupby('Country')['InDegree'].sum().nlargest(5).index.tolist()\nau_in_degrees = df[df['Country'] == 'AU'].groupby('Country')['InDegree'].sum().values[0]\n\n# Find the countries with the most similar in-degrees to Australia\ndf['InDegreeDiff'] = (df.groupby('Country')['InDegree'].transform('sum') - au_in_degrees).abs()\nsimilar_countries = df.loc[df['Country'] != 'AU'].groupby('Country')['InDegreeDiff'].mean().nsmallest(2).index.tolist()\n\n# Combine the top countries, Australia, and similar countries\nselected_countries = top_countries + ['AU'] + similar_countries\n\n# Filter DataFrame for selected countries\ndf = df[df['Country'].isin(selected_countries)]\n\n# Pivot the DataFrame for plotting\ndf_pivot = df.pivot_table(index='Country', columns='Year', values='InDegree', fill_value=0)\n\n# Plotting the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot each year as a separate bar in the clustered column chart\nbar_width = 0.15\nyears = sorted(df['Year'].unique())\npositions = range(len(df_pivot.index))\nfor i, year in enumerate(years):\n    year_data = df_pivot[year]\n    bar_positions = [p + i * bar_width for p in positions]\n    country_colors = [df[df['Country'] == country]['SubregionColour'].values[0] for country in df_pivot.index]\n    bars = ax.bar(bar_positions, year_data, width=bar_width, label=year, color=country_colors, edgecolor='black')  # Add edgecolor for border\n\n# Set the position of the x ticks\nax.set_xticks([p + (len(years) - 1) * bar_width / 2 for p in positions])\nax.set_xticklabels(df_pivot.index, rotation=90)\n\n# Adding labels and title\nax.set_xlabel('Country')\nax.set_ylabel('Weighted In Degree')\nax.set_title('Weighted In Degree of Countries by Year\\nTop 5 nodes + Australia and 2 context nodes shown\\nColumns within a country cluster from left to right show total number of posts in 2016, 2017, 2019, 2021, and 2023 respectively')\n\n# Create a custom legend for subregions\nsubregions = df[['Subregion', 'SubregionColour']].drop_duplicates().sort_values('Subregion')\nhandles = [plt.Line2D([0], [0], marker='o', color=row['SubregionColour'], markersize=10, linestyle='') for _, row in subregions.iterrows()]\nlabels = subregions['Subregion'].tolist()\nax.legend(handles, labels, title='Subregions', loc='center left', bbox_to_anchor=(1, 0.5))\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n</pre> # Function to get nodes with incoming edges and their weighted in-degrees for a given graph def get_weighted_in_degrees(graph):     weighted_in_degrees = {node: graph.in_degree(node, weight='weight') for node in graph.nodes if graph.in_degree(node) &gt; 0}     return weighted_in_degrees  # Initialise a dictionary to store the results in_degrees_by_year = {}  # Compute the weighted in-degrees for each year for year, graph in graphs.items():     in_degrees_by_year[year] = get_weighted_in_degrees(graph)  # Collect data for plotting data = [] for year, in_degrees in in_degrees_by_year.items():     for node, in_degree in in_degrees.items():         subregion = graph.nodes[node]['subregion']         subregion_colour = graph.nodes[node]['subregionColour']         data.append([year, node, in_degree, subregion, subregion_colour])  # Create a DataFrame df = pd.DataFrame(data, columns=['Year', 'Country', 'InDegree', 'Subregion', 'SubregionColour'])  # Identify top 4 countries by in-degree and countries with similar in-degrees to Australia top_countries = df.groupby('Country')['InDegree'].sum().nlargest(5).index.tolist() au_in_degrees = df[df['Country'] == 'AU'].groupby('Country')['InDegree'].sum().values[0]  # Find the countries with the most similar in-degrees to Australia df['InDegreeDiff'] = (df.groupby('Country')['InDegree'].transform('sum') - au_in_degrees).abs() similar_countries = df.loc[df['Country'] != 'AU'].groupby('Country')['InDegreeDiff'].mean().nsmallest(2).index.tolist()  # Combine the top countries, Australia, and similar countries selected_countries = top_countries + ['AU'] + similar_countries  # Filter DataFrame for selected countries df = df[df['Country'].isin(selected_countries)]  # Pivot the DataFrame for plotting df_pivot = df.pivot_table(index='Country', columns='Year', values='InDegree', fill_value=0)  # Plotting the data fig, ax = plt.subplots(figsize=(15, 10))  # Plot each year as a separate bar in the clustered column chart bar_width = 0.15 years = sorted(df['Year'].unique()) positions = range(len(df_pivot.index)) for i, year in enumerate(years):     year_data = df_pivot[year]     bar_positions = [p + i * bar_width for p in positions]     country_colors = [df[df['Country'] == country]['SubregionColour'].values[0] for country in df_pivot.index]     bars = ax.bar(bar_positions, year_data, width=bar_width, label=year, color=country_colors, edgecolor='black')  # Add edgecolor for border  # Set the position of the x ticks ax.set_xticks([p + (len(years) - 1) * bar_width / 2 for p in positions]) ax.set_xticklabels(df_pivot.index, rotation=90)  # Adding labels and title ax.set_xlabel('Country') ax.set_ylabel('Weighted In Degree') ax.set_title('Weighted In Degree of Countries by Year\\nTop 5 nodes + Australia and 2 context nodes shown\\nColumns within a country cluster from left to right show total number of posts in 2016, 2017, 2019, 2021, and 2023 respectively')  # Create a custom legend for subregions subregions = df[['Subregion', 'SubregionColour']].drop_duplicates().sort_values('Subregion') handles = [plt.Line2D([0], [0], marker='o', color=row['SubregionColour'], markersize=10, linestyle='') for _, row in subregions.iterrows()] labels = subregions['Subregion'].tolist() ax.legend(handles, labels, title='Subregions', loc='center left', bbox_to_anchor=(1, 0.5))  # Display the plot plt.tight_layout() plt.show() In\u00a0[18]: Copied! <pre># Grouping by 'COUNTRY' and 'Year'\npost_counts_summary = post_counts_df.groupby(['COUNTRY', 'Year'])['POST COUNT'].sum().reset_index()\n\n# Join the aggregated data with the countries data to get the country names\npost_counts_summary = post_counts_summary.merge(countries_df[['id', 'name']], left_on='COUNTRY', right_on='id', how='left')\n\n# Top 10 countries by post count for each year\ntop_countries_by_year = (\n    post_counts_summary\n      .groupby('Year', group_keys=False)\n      .apply(lambda x: x.nlargest(10, 'POST COUNT'), include_groups=False).reset_index(drop=True)\n)\n\n# post_counts_summary, top_countries_by_year.head()\n</pre> # Grouping by 'COUNTRY' and 'Year' post_counts_summary = post_counts_df.groupby(['COUNTRY', 'Year'])['POST COUNT'].sum().reset_index()  # Join the aggregated data with the countries data to get the country names post_counts_summary = post_counts_summary.merge(countries_df[['id', 'name']], left_on='COUNTRY', right_on='id', how='left')  # Top 10 countries by post count for each year top_countries_by_year = (     post_counts_summary       .groupby('Year', group_keys=False)       .apply(lambda x: x.nlargest(10, 'POST COUNT'), include_groups=False).reset_index(drop=True) )  # post_counts_summary, top_countries_by_year.head() In\u00a0[19]: Copied! <pre>australia_posts = post_counts_df[post_counts_df['COUNTRY'] == 'AU']\n\naustralia_posts = australia_posts.merge(countries_df[['id', 'region', 'regionColour']], left_on='POST COUNTRY', right_on='id', how='left')\n\naustralia_posts_summary = australia_posts.groupby(['Year', 'region', 'regionColour'])['POST COUNT'].sum().reset_index()\n\naustralia_posts_summary\n\n# Pivot the DataFrame to have years as index and regions as columns\npivot_df = australia_posts_summary.pivot(index='Year', columns='region', values='POST COUNT').fillna(0)\n\n# Plotting the bar chart with legend outside and total count on top of each bar\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plotting the stacked bar chart\npivot_df.plot(kind='bar', stacked=True, ax=ax, color=australia_posts_summary.drop_duplicates('region')['regionColour'])\n\n# Calculating the total post count for each year and placing the label\ntotals = pivot_df.sum(axis=1)\nfor i, total in enumerate(totals):\n    ax.text(i, total + 2, str(total), ha='center', weight='bold')\n\n# Setting the title and labels\nax.set_title(\"Australia's Diplomatic Presence by Region (2016-2023)\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Post Count\")\n\n# Moving the legend outside the plot\nax.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.show()\n\n#fig.savefig(\"Australias_Diplomatic_Presence_2016_2023.png\", bbox_inches='tight')\n</pre> australia_posts = post_counts_df[post_counts_df['COUNTRY'] == 'AU']  australia_posts = australia_posts.merge(countries_df[['id', 'region', 'regionColour']], left_on='POST COUNTRY', right_on='id', how='left')  australia_posts_summary = australia_posts.groupby(['Year', 'region', 'regionColour'])['POST COUNT'].sum().reset_index()  australia_posts_summary  # Pivot the DataFrame to have years as index and regions as columns pivot_df = australia_posts_summary.pivot(index='Year', columns='region', values='POST COUNT').fillna(0)  # Plotting the bar chart with legend outside and total count on top of each bar fig, ax = plt.subplots(figsize=(12, 8))  # Plotting the stacked bar chart pivot_df.plot(kind='bar', stacked=True, ax=ax, color=australia_posts_summary.drop_duplicates('region')['regionColour'])  # Calculating the total post count for each year and placing the label totals = pivot_df.sum(axis=1) for i, total in enumerate(totals):     ax.text(i, total + 2, str(total), ha='center', weight='bold')  # Setting the title and labels ax.set_title(\"Australia's Diplomatic Presence by Region (2016-2023)\") ax.set_xlabel(\"Year\") ax.set_ylabel(\"Post Count\")  # Moving the legend outside the plot ax.legend(title='Region', bbox_to_anchor=(1.05, 1), loc='upper left')  plt.show()  #fig.savefig(\"Australias_Diplomatic_Presence_2016_2023.png\", bbox_inches='tight')  In\u00a0[20]: Copied! <pre># Filter data for the selected countries\nselected_countries_posts = post_counts_df[post_counts_df['COUNTRY'].isin(['AU', 'CN', 'US', 'IN', 'ID'])]\n\n# Create an empty DataFrame to store new posts by year\nnew_posts_by_year = pd.DataFrame(columns=['COUNTRY', 'POST COUNTRY', 'Year'])\n\n# Determine new posts by year for each country, starting from 2017\nfor country in ['AU', 'CN', 'US', 'IN', 'ID']:\n    country_posts = selected_countries_posts[selected_countries_posts['COUNTRY'] == country]\n    for year in sorted(country_posts['Year'].unique()):\n        if year == 2016:\n            continue  # Skip the year 2016\n        current_year_posts = set(country_posts[country_posts['Year'] == year]['POST COUNTRY'])\n        previous_year_posts = set(country_posts[country_posts['Year'] &lt; year]['POST COUNTRY'])\n        new_posts = current_year_posts - previous_year_posts\n        new_posts_by_year = pd.concat(\n            [new_posts_by_year, pd.DataFrame({'COUNTRY': country, 'POST COUNTRY': list(new_posts), 'Year': year})],\n            ignore_index=True\n        )\n\n# Merge the new posts by year with countries to get subregion and subregionColour for POST COUNTRY\nnew_posts_with_subregion = new_posts_by_year.merge(\n    countries_df[['id', 'region', 'regionColour', 'subregion', 'subregionColour']], \n    left_on='POST COUNTRY', \n    right_on='id', \n    how='left'\n)\n\n# Drop the 'id' column as it's redundant now\nnew_posts_with_subregion.drop(columns=['id'], inplace=True)\n\n# Group by country and region and count the number of new posts\ngrouped_new_posts = new_posts_with_subregion.groupby(['COUNTRY', 'region', 'regionColour']).size().reset_index(name='New Posts Count')\n\n# Pivot the data for plotting\npivot_data_grouped = grouped_new_posts.pivot_table(\n    index='COUNTRY',\n    columns='region',\n    values='New Posts Count',\n    fill_value=0\n)\n\n# Create a color map for regions\nregion_colors_map = dict(zip(grouped_new_posts['region'], grouped_new_posts['regionColour']))\n\n# Plot the data\nfig, ax = plt.subplots(figsize=(7, 4))\npivot_data_grouped.plot(kind='bar', stacked=True, color=[region_colors_map[region] for region in pivot_data_grouped.columns], ax=ax)\n\n# Set labels and title\nax.set_xlabel('Country')\nax.set_ylabel('New Posts Opened Since 2017')\nax.set_title('New Posts Opened by Selected Countries by Region Since 2017')\n\n# Change y-axis to display whole numbers only\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n# Create a legend for regions\nhandles, labels = ax.get_legend_handles_labels()\nunique_regions = grouped_new_posts[['region', 'regionColour']].drop_duplicates()\ncolor_patches = [plt.Line2D([0], [0], color=row['regionColour'], lw=4) for _, row in unique_regions.iterrows()]\nunique_labels = unique_regions['region'].tolist()\nax.legend(color_patches, unique_labels, title='Region')\n\nplt.show()\n</pre> # Filter data for the selected countries selected_countries_posts = post_counts_df[post_counts_df['COUNTRY'].isin(['AU', 'CN', 'US', 'IN', 'ID'])]  # Create an empty DataFrame to store new posts by year new_posts_by_year = pd.DataFrame(columns=['COUNTRY', 'POST COUNTRY', 'Year'])  # Determine new posts by year for each country, starting from 2017 for country in ['AU', 'CN', 'US', 'IN', 'ID']:     country_posts = selected_countries_posts[selected_countries_posts['COUNTRY'] == country]     for year in sorted(country_posts['Year'].unique()):         if year == 2016:             continue  # Skip the year 2016         current_year_posts = set(country_posts[country_posts['Year'] == year]['POST COUNTRY'])         previous_year_posts = set(country_posts[country_posts['Year'] &lt; year]['POST COUNTRY'])         new_posts = current_year_posts - previous_year_posts         new_posts_by_year = pd.concat(             [new_posts_by_year, pd.DataFrame({'COUNTRY': country, 'POST COUNTRY': list(new_posts), 'Year': year})],             ignore_index=True         )  # Merge the new posts by year with countries to get subregion and subregionColour for POST COUNTRY new_posts_with_subregion = new_posts_by_year.merge(     countries_df[['id', 'region', 'regionColour', 'subregion', 'subregionColour']],      left_on='POST COUNTRY',      right_on='id',      how='left' )  # Drop the 'id' column as it's redundant now new_posts_with_subregion.drop(columns=['id'], inplace=True)  # Group by country and region and count the number of new posts grouped_new_posts = new_posts_with_subregion.groupby(['COUNTRY', 'region', 'regionColour']).size().reset_index(name='New Posts Count')  # Pivot the data for plotting pivot_data_grouped = grouped_new_posts.pivot_table(     index='COUNTRY',     columns='region',     values='New Posts Count',     fill_value=0 )  # Create a color map for regions region_colors_map = dict(zip(grouped_new_posts['region'], grouped_new_posts['regionColour']))  # Plot the data fig, ax = plt.subplots(figsize=(7, 4)) pivot_data_grouped.plot(kind='bar', stacked=True, color=[region_colors_map[region] for region in pivot_data_grouped.columns], ax=ax)  # Set labels and title ax.set_xlabel('Country') ax.set_ylabel('New Posts Opened Since 2017') ax.set_title('New Posts Opened by Selected Countries by Region Since 2017')  # Change y-axis to display whole numbers only ax.yaxis.set_major_locator(MaxNLocator(integer=True))  # Create a legend for regions handles, labels = ax.get_legend_handles_labels() unique_regions = grouped_new_posts[['region', 'regionColour']].drop_duplicates() color_patches = [plt.Line2D([0], [0], color=row['regionColour'], lw=4) for _, row in unique_regions.iterrows()] unique_labels = unique_regions['region'].tolist() ax.legend(color_patches, unique_labels, title='Region')  plt.show() In\u00a0[14]: Copied! <pre>## Network Power\n\n# Variables in dataset \n\n# countryA\n# countryB\n# btN bilateralTradeNormValue\n# btR bilateralTradeRawValue\n# fciN foreignCapitalInvestmentNormValue\n# fciR foreignCapitalInvestmentRawValue\n# osiR onlineSearchInterestRawValue\n# mdR migrantDestinationsRawValue\n# mdN migrantDestinationsNormValue\n# tdR travelDestinationsRawValue\n# tdN travelDestinationsNormValue\n# fmfR foreignMediaFlowsRawValue\n# atN armsTradeNormValue\n# atR armsTradeRawValue\n# jtN jointTrainingNormValue\n# jtR jointTrainingRawValue\n# dedN defenceDialoguesNormValue\n# dedR defenceDialoguesRawValue\n# didN diplomaticDialoguesNormValue\n# didN diplomaticDialoguesRawValue\n</pre> ## Network Power  # Variables in dataset   # countryA # countryB # btN bilateralTradeNormValue # btR bilateralTradeRawValue # fciN foreignCapitalInvestmentNormValue # fciR foreignCapitalInvestmentRawValue # osiR onlineSearchInterestRawValue # mdR migrantDestinationsRawValue # mdN migrantDestinationsNormValue # tdR travelDestinationsRawValue # tdN travelDestinationsNormValue # fmfR foreignMediaFlowsRawValue # atN armsTradeNormValue # atR armsTradeRawValue # jtN jointTrainingNormValue # jtR jointTrainingRawValue # dedN defenceDialoguesNormValue # dedR defenceDialoguesRawValue # didN diplomaticDialoguesNormValue # didN diplomaticDialoguesRawValue In\u00a0[21]: Copied! <pre># online search interest\n\n# Filter the network_power_df for non-NaN values in 'onlineSearchInterestRawValue'\nonlineSearchInterest = network_power_df.dropna(subset=['onlineSearchInterestRawValue'])\n\nG_osiR = nx.from_pandas_edgelist(\n    df=onlineSearchInterest,\n    source='countryA',\n    target='countryB',\n    edge_attr=['onlineSearchInterestRawValue'],\n    create_using=nx.DiGraph()\n)\n\n# Rename edge attribute 'onlineSearchInterestRawValue' to 'weight'\nfor u, v, d in G_osiR.edges(data=True):\n    d['weight'] = d.pop('onlineSearchInterestRawValue')\n\n# Confirming the update by checking a few edge details\nedge_details_updated = list(G_osiR.edges(data=True))[:5]\nedge_details_updated\n\n# Correct the index column and add attributes\ncountry_info = countries_df.set_index('id').to_dict('index')\n\n# Filter country_attributes_df for the year 2023\ncountry_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023].set_index('COUNTRY')\n\n# Create a dictionary for country attributes\ncountry_attributes_info = country_attributes_2023[['POPULATION (M)', 'GDP (B, USD)']].to_dict('index')\n\n# Update the graph with the correct attributes\nfor node in G_osiR.nodes():\n    if node in country_info:\n        G_osiR.nodes[node]['subregion'] = country_info[node].get('subregion')\n        G_osiR.nodes[node]['subregion_color'] = country_info[node].get('subregionColour')\n    if node in country_attributes_info:\n        G_osiR.nodes[node]['population_2023'] = country_attributes_info[node].get('POPULATION (M)')\n        G_osiR.nodes[node]['GDP_2023'] = country_attributes_info[node].get('GDP (B, USD)')\n\n# Check the number of nodes and edges to confirm creation\nnum_nodes = G_osiR.number_of_nodes()\nnum_edges = G_osiR.number_of_edges()\n\n# Show a few edge details and node attributes to confirm everything has been added correctly\nedge_details = list(G_osiR.edges(data=True))[:5]\nnode_attributes = {node: G_osiR.nodes[node] for node in list(G_osiR.nodes)[:5]}\n\n# num_nodes, num_edges, edge_details, node_attributes\n</pre> # online search interest  # Filter the network_power_df for non-NaN values in 'onlineSearchInterestRawValue' onlineSearchInterest = network_power_df.dropna(subset=['onlineSearchInterestRawValue'])  G_osiR = nx.from_pandas_edgelist(     df=onlineSearchInterest,     source='countryA',     target='countryB',     edge_attr=['onlineSearchInterestRawValue'],     create_using=nx.DiGraph() )  # Rename edge attribute 'onlineSearchInterestRawValue' to 'weight' for u, v, d in G_osiR.edges(data=True):     d['weight'] = d.pop('onlineSearchInterestRawValue')  # Confirming the update by checking a few edge details edge_details_updated = list(G_osiR.edges(data=True))[:5] edge_details_updated  # Correct the index column and add attributes country_info = countries_df.set_index('id').to_dict('index')  # Filter country_attributes_df for the year 2023 country_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023].set_index('COUNTRY')  # Create a dictionary for country attributes country_attributes_info = country_attributes_2023[['POPULATION (M)', 'GDP (B, USD)']].to_dict('index')  # Update the graph with the correct attributes for node in G_osiR.nodes():     if node in country_info:         G_osiR.nodes[node]['subregion'] = country_info[node].get('subregion')         G_osiR.nodes[node]['subregion_color'] = country_info[node].get('subregionColour')     if node in country_attributes_info:         G_osiR.nodes[node]['population_2023'] = country_attributes_info[node].get('POPULATION (M)')         G_osiR.nodes[node]['GDP_2023'] = country_attributes_info[node].get('GDP (B, USD)')  # Check the number of nodes and edges to confirm creation num_nodes = G_osiR.number_of_nodes() num_edges = G_osiR.number_of_edges()  # Show a few edge details and node attributes to confirm everything has been added correctly edge_details = list(G_osiR.edges(data=True))[:5] node_attributes = {node: G_osiR.nodes[node] for node in list(G_osiR.nodes)[:5]}  # num_nodes, num_edges, edge_details, node_attributes In\u00a0[22]: Copied! <pre># Set display options to show the entire DataFrame\npd.set_option('display.max_rows', None)  # Show all rows\npd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.width', None)  # Adjust width to fit the content\npd.set_option('display.max_colwidth', None)  # Adjust column width to fit the content\n</pre> # Set display options to show the entire DataFrame pd.set_option('display.max_rows', None)  # Show all rows pd.set_option('display.max_columns', None)  # Show all columns pd.set_option('display.width', None)  # Adjust width to fit the content pd.set_option('display.max_colwidth', None)  # Adjust column width to fit the content In\u00a0[23]: Copied! <pre>pagerank = nx.pagerank(G_osiR, weight='weight')\n\n# Sort and display the top 5 nodes by PageRank\nsorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\ntop_6_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[:6]]\n\ntop_6_pagerank\n\nbottom_5_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[-5:]]\n\nsorted_pagerank\n</pre> pagerank = nx.pagerank(G_osiR, weight='weight')  # Sort and display the top 5 nodes by PageRank sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True) top_6_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[:6]]  top_6_pagerank  bottom_5_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[-5:]]  sorted_pagerank Out[23]: <pre>[('JP', 0.1541194921094484),\n ('US', 0.11375317662150906),\n ('CN', 0.10724435439656198),\n ('IN', 0.09396449697743527),\n ('SK', 0.06718038184644991),\n ('AU', 0.053082429271692926),\n ('TH', 0.03989958690025213),\n ('RU', 0.03582990475151742),\n ('SG', 0.03321633904969601),\n ('VN', 0.029661697589070023),\n ('NZ', 0.026287177850123078),\n ('PK', 0.025890451512565547),\n ('PH', 0.024900480122836847),\n ('ID', 0.024104082042803565),\n ('MA', 0.022220530029223938),\n ('TW', 0.021622619487506625),\n ('BA', 0.01915908405209723),\n ('MY', 0.01756467462829505),\n ('NP', 0.017186180133021936),\n ('NK', 0.014624922978226283),\n ('SL', 0.012589242707708729),\n ('CB', 0.011708079119043098),\n ('MO', 0.01123570143128116),\n ('LA', 0.008939914498372411),\n ('BR', 0.007211466332545399),\n ('PG', 0.006803533560716264)]</pre> In\u00a0[25]: Copied! <pre>hubs, authorities = nx.hits(G_osiR)\n# Sort and display the top 6 hubs and authorities\nsorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)\nsorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n\ntop_6_hubs = [(node, round(score, 2)) for node, score in sorted_hubs[:6]]\ntop_6_authorities = [(node, round(score, 2)) for node, score in sorted_authorities[:6]]\n\nsorted_hubs, sorted_authorities\n</pre> hubs, authorities = nx.hits(G_osiR) # Sort and display the top 6 hubs and authorities sorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True) sorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)  top_6_hubs = [(node, round(score, 2)) for node, score in sorted_hubs[:6]] top_6_authorities = [(node, round(score, 2)) for node, score in sorted_authorities[:6]]  sorted_hubs, sorted_authorities Out[25]: <pre>([('MY', 0.058774123147069895),\n  ('TH', 0.05591133323686764),\n  ('VN', 0.05430788341370337),\n  ('MO', 0.05229496468801094),\n  ('PH', 0.050320442265359235),\n  ('NP', 0.045509864478488676),\n  ('SK', 0.04474828991208618),\n  ('SL', 0.04469339996848052),\n  ('TW', 0.04376676389571327),\n  ('RU', 0.04372511993130162),\n  ('CB', 0.043692761423511625),\n  ('PK', 0.04237587083264506),\n  ('BA', 0.04118686981405634),\n  ('MA', 0.03902311139026791),\n  ('US', 0.038280840771051414),\n  ('LA', 0.03559357372426088),\n  ('SG', 0.0330572726959318),\n  ('NZ', 0.03299423431697112),\n  ('AU', 0.0329712966595284),\n  ('IN', 0.032449279136158915),\n  ('ID', 0.030335611522782975),\n  ('PG', 0.030066250998405455),\n  ('BR', 0.026773396688816887),\n  ('CN', 0.02406048790040514),\n  ('JP', 0.02308695718812486),\n  ('NK', -0.0)],\n [('JP', 0.23849583734613491),\n  ('IN', 0.13999737554451572),\n  ('CN', 0.11919197286555143),\n  ('US', 0.1118559204734656),\n  ('TH', 0.061904339427653395),\n  ('AU', 0.05229199540417414),\n  ('SG', 0.03556343210975368),\n  ('SK', 0.02956812121200929),\n  ('VN', 0.027511435732626523),\n  ('RU', 0.026107524608917534),\n  ('ID', 0.024127656343898473),\n  ('MA', 0.022921025254196804),\n  ('NZ', 0.019210261822382047),\n  ('PH', 0.019075370719477302),\n  ('PK', 0.016113135123391412),\n  ('BA', 0.012769674346497124),\n  ('MY', 0.00984855660893087),\n  ('NP', 0.0071553779149472205),\n  ('CB', 0.0059887160864549515),\n  ('SL', 0.005223348949739134),\n  ('NK', 0.004421191346948612),\n  ('TW', 0.004222795334308151),\n  ('MO', 0.00242151241725916),\n  ('LA', 0.0023829014983536285),\n  ('BR', 0.0008822571736626918),\n  ('PG', 0.0007482643347500311)])</pre> In\u00a0[26]: Copied! <pre># Perform Greedy Modularity Maximization community detection\ngreedy_modularity_communities = nx_comm.greedy_modularity_communities(G_osiR, weight='weight')\ngm_communities = [list(community) for community in greedy_modularity_communities]\n\n# Perform Girvan-Newman community detection\ngirvan_newman_communities = list(nx_comm.girvan_newman(G_osiR))\ngn_communities_level_1 = [list(community) for community in girvan_newman_communities[0]]\n\n# Convert communities to dictionaries for easier comparison\ngn_communities_dict = {i: community for i, community in enumerate(gn_communities_level_1)}\ngm_communities_dict = {i: community for i, community in enumerate(gm_communities)}\n\ngn_communities_dict, gm_communities_dict\n</pre> # Perform Greedy Modularity Maximization community detection greedy_modularity_communities = nx_comm.greedy_modularity_communities(G_osiR, weight='weight') gm_communities = [list(community) for community in greedy_modularity_communities]  # Perform Girvan-Newman community detection girvan_newman_communities = list(nx_comm.girvan_newman(G_osiR)) gn_communities_level_1 = [list(community) for community in girvan_newman_communities[0]]  # Convert communities to dictionaries for easier comparison gn_communities_dict = {i: community for i, community in enumerate(gn_communities_level_1)} gm_communities_dict = {i: community for i, community in enumerate(gm_communities)}  gn_communities_dict, gm_communities_dict Out[26]: <pre>({0: ['TW',\n   'MY',\n   'PH',\n   'NZ',\n   'TH',\n   'AU',\n   'RU',\n   'NP',\n   'MA',\n   'SG',\n   'MO',\n   'SK',\n   'ID',\n   'CN',\n   'BA',\n   'CB',\n   'IN',\n   'BR',\n   'LA',\n   'US',\n   'PG',\n   'JP',\n   'SL',\n   'VN',\n   'PK'],\n  1: ['NK']},\n {0: ['TW', 'LA', 'NK', 'JP', 'MY', 'CB', 'TH', 'VN', 'MO', 'SK', 'CN'],\n  1: ['AU', 'RU', 'US', 'PG', 'PH', 'NZ'],\n  2: ['BA', 'IN', 'PK', 'SL', 'NP'],\n  3: ['MA', 'ID', 'SG', 'BR']})</pre> In\u00a0[27]: Copied! <pre>G_osiR['AU'].items()\nG_osiR.in_edges('AU', data=True)\nsorted(G_osiR['AU'].items(), key=lambda x: x[1]['weight'], reverse=True)\nsorted(G_osiR.in_edges('AU', data=True), key=lambda x: x[2]['weight'], reverse=True)\n</pre> G_osiR['AU'].items() G_osiR.in_edges('AU', data=True) sorted(G_osiR['AU'].items(), key=lambda x: x[1]['weight'], reverse=True) sorted(G_osiR.in_edges('AU', data=True), key=lambda x: x[2]['weight'], reverse=True) Out[27]: <pre>[('PG', 'AU', {'weight': 0.2894697977}),\n ('NZ', 'AU', {'weight': 0.2866237631}),\n ('US', 'AU', {'weight': 0.0819672131}),\n ('IN', 'AU', {'weight': 0.0817810892}),\n ('SK', 'AU', {'weight': 0.0701915709}),\n ('SL', 'AU', {'weight': 0.0656634747}),\n ('NP', 'AU', {'weight': 0.0590731625}),\n ('JP', 'AU', {'weight': 0.0484212176}),\n ('CN', 'AU', {'weight': 0.046181771}),\n ('PK', 'AU', {'weight': 0.045176588}),\n ('SG', 'AU', {'weight': 0.0441430333}),\n ('RU', 'AU', {'weight': 0.0425777863}),\n ('PH', 'AU', {'weight': 0.0418154408}),\n ('BA', 'AU', {'weight': 0.0340479193}),\n ('MO', 'AU', {'weight': 0.0329777971}),\n ('TW', 'AU', {'weight': 0.025949103}),\n ('VN', 'AU', {'weight': 0.0241400746}),\n ('BR', 'AU', {'weight': 0.0223330562}),\n ('MA', 'AU', {'weight': 0.0219930146}),\n ('TH', 'AU', {'weight': 0.0172032193}),\n ('CB', 'AU', {'weight': 0.0145108098}),\n ('ID', 'AU', {'weight': 0.0140764695}),\n ('LA', 'AU', {'weight': 0.0108968072}),\n ('MY', 'AU', {'weight': 0.0096485183})]</pre> In\u00a0[28]: Copied! <pre># Get the top 5 targets of Australia's searches\ntop_5_targets_AU = sorted(G_osiR['AU'].items(), key=lambda x: x[1]['weight'], reverse=True)[:5]\n\n# Get the top 5 sources of searches to Australia\ntop_5_sources_AU = sorted(G_osiR.in_edges('AU', data=True), key=lambda x: x[2]['weight'], reverse=True)[:5]\n\nprint(\"Top 5 Targets of AU's Searches:\", top_5_targets_AU)\nprint(\"Top 5 Sources of Searches to AU:\", top_5_sources_AU)\n\n# Extract data for visualization\ntop_5_targets_AU_df = pd.DataFrame(top_5_targets_AU, columns=['Target', 'Data'])\ntop_5_targets_AU_df['Weight'] = top_5_targets_AU_df['Data'].apply(lambda x: x['weight'])\ntop_5_targets_AU_df = top_5_targets_AU_df.drop(columns=['Data'])\n\ntop_5_sources_AU_df = pd.DataFrame(top_5_sources_AU, columns=['Source', 'Target', 'Data'])\ntop_5_sources_AU_df['Weight'] = top_5_sources_AU_df['Data'].apply(lambda x: x['weight'])\ntop_5_sources_AU_df = top_5_sources_AU_df.drop(columns=['Data'])\n\n# Plotting the data\nplt.figure(figsize=(14, 6))\n\n# Plot for top 5 targets of Australia\nplt.subplot(1, 2, 1)\nplt.barh(top_5_targets_AU_df['Target'], top_5_targets_AU_df['Weight'], color='skyblue')\nplt.xlabel('Online Search Interest Raw Value')\nplt.xlim(0, 0.3)\nplt.title('Top 5 Targets of Australia\\'s Searches')\nplt.gca().invert_yaxis()\n\n# Plot for top 5 sources of searches to Australia\nplt.subplot(1, 2, 2)\nplt.barh(top_5_sources_AU_df['Source'], top_5_sources_AU_df['Weight'], color='salmon')\nplt.xlabel('Online Search Interest Raw Value')\nplt.xlim(0, 0.3)\nplt.title('Top 5 Sources of Searches to Australia')\nplt.gca().invert_yaxis()\n\nplt.tight_layout()\nplt.show()\n</pre> # Get the top 5 targets of Australia's searches top_5_targets_AU = sorted(G_osiR['AU'].items(), key=lambda x: x[1]['weight'], reverse=True)[:5]  # Get the top 5 sources of searches to Australia top_5_sources_AU = sorted(G_osiR.in_edges('AU', data=True), key=lambda x: x[2]['weight'], reverse=True)[:5]  print(\"Top 5 Targets of AU's Searches:\", top_5_targets_AU) print(\"Top 5 Sources of Searches to AU:\", top_5_sources_AU)  # Extract data for visualization top_5_targets_AU_df = pd.DataFrame(top_5_targets_AU, columns=['Target', 'Data']) top_5_targets_AU_df['Weight'] = top_5_targets_AU_df['Data'].apply(lambda x: x['weight']) top_5_targets_AU_df = top_5_targets_AU_df.drop(columns=['Data'])  top_5_sources_AU_df = pd.DataFrame(top_5_sources_AU, columns=['Source', 'Target', 'Data']) top_5_sources_AU_df['Weight'] = top_5_sources_AU_df['Data'].apply(lambda x: x['weight']) top_5_sources_AU_df = top_5_sources_AU_df.drop(columns=['Data'])  # Plotting the data plt.figure(figsize=(14, 6))  # Plot for top 5 targets of Australia plt.subplot(1, 2, 1) plt.barh(top_5_targets_AU_df['Target'], top_5_targets_AU_df['Weight'], color='skyblue') plt.xlabel('Online Search Interest Raw Value') plt.xlim(0, 0.3) plt.title('Top 5 Targets of Australia\\'s Searches') plt.gca().invert_yaxis()  # Plot for top 5 sources of searches to Australia plt.subplot(1, 2, 2) plt.barh(top_5_sources_AU_df['Source'], top_5_sources_AU_df['Weight'], color='salmon') plt.xlabel('Online Search Interest Raw Value') plt.xlim(0, 0.3) plt.title('Top 5 Sources of Searches to Australia') plt.gca().invert_yaxis()  plt.tight_layout() plt.show() <pre>Top 5 Targets of AU's Searches: [('US', {'weight': 0.2100456621}), ('IN', {'weight': 0.1417315785}), ('NZ', {'weight': 0.1306294207}), ('JP', {'weight': 0.1043065628}), ('CN', {'weight': 0.0962485451})]\nTop 5 Sources of Searches to AU: [('PG', 'AU', {'weight': 0.2894697977}), ('NZ', 'AU', {'weight': 0.2866237631}), ('US', 'AU', {'weight': 0.0819672131}), ('IN', 'AU', {'weight': 0.0817810892}), ('SK', 'AU', {'weight': 0.0701915709})]\n</pre> In\u00a0[29]: Copied! <pre>top_edges = sorted(G_osiR.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:25]\n\nsubgraph = nx.DiGraph()\nfor u, v, data in top_edges:\n    subgraph.add_edge(u, v, weight=data['weight'])\n\n\n\ndef update_graph_colors(G, countries_df):\n    # Extract nodes data from the graph\n    nodes_data = []\n    for node, data in G.nodes(data=True):\n        nodes_data.append({'node': node, 'label': data.get('label')})\n    \n    nodes_df = pd.DataFrame(nodes_data)\n    \n    # Join the nodes_df with countries_df to get the color information using 'id'\n    merged_df = nodes_df.merge(countries_df[['id', 'subregionColour']], left_on='node', right_on='id', how='left')\n    \n    # Update the graph with the color information\n    for node in G.nodes:\n        node_id = node\n        color = merged_df.loc[merged_df['node'] == node_id, 'subregionColour'].values\n        G.nodes[node]['color'] = color[0] if len(color) &gt; 0 else '#000000'  # Default to black if no color found\n\n    return G\n\nG_osiR = update_graph_colors(G_osiR, countries_df)\n\n# Extract node colors and edge weights again after merging color information\nnode_colors = [G_osiR.nodes[node]['color'] for node in subgraph.nodes]\nedge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges]\n\n# Normalize edge weights for plotting\nmax_weight = max(edge_weights)\nmin_weight = min(edge_weights)\nedge_thickness = [(weight - min_weight) / (max_weight - min_weight) * 5 + 1 for weight in edge_weights]\n\n# Filter the subregions to only include those relevant to the top 25 edges and nodes\nrelevant_nodes = set(subgraph.nodes)\nrelevant_subregions = countries_df[countries_df['id'].isin(relevant_nodes)]['subregion'].unique()\n\n# Create the legend for the relevant subregions\nrelevant_subregion_colors = countries_df[countries_df['subregion'].isin(relevant_subregions)][['subregion', 'subregionColour']].drop_duplicates()\nhandles = [mpatches.Patch(color=row['subregionColour'], label=row['subregion']) for idx, row in relevant_subregion_colors.iterrows()]\n\n# Plot the graph with updated colors and arrows for directionality\nplt.figure(figsize=(17, 8))\npos = nx.spring_layout(subgraph, seed=42)\nnx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, node_size=500)\nnx.draw_networkx_edges(subgraph, pos, width=edge_thickness, arrowstyle='-&gt;', arrowsize=10)\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# Add a title and subtitle as part of the main title\nplt.title('Top 25 Online Search Interest Flows\\nLine thickness represents the percentage of searches from country A to B')\n\n# Add the legend for the relevant subregions inside the graph border\nplt.legend(handles=handles, title='Subregions', loc='upper right', bbox_to_anchor=(0.9, 0.9))\nplt.savefig('top_25_online_search_interest_flows.png', format='png', bbox_inches='tight')\nplt.show()\n</pre> top_edges = sorted(G_osiR.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:25]  subgraph = nx.DiGraph() for u, v, data in top_edges:     subgraph.add_edge(u, v, weight=data['weight'])    def update_graph_colors(G, countries_df):     # Extract nodes data from the graph     nodes_data = []     for node, data in G.nodes(data=True):         nodes_data.append({'node': node, 'label': data.get('label')})          nodes_df = pd.DataFrame(nodes_data)          # Join the nodes_df with countries_df to get the color information using 'id'     merged_df = nodes_df.merge(countries_df[['id', 'subregionColour']], left_on='node', right_on='id', how='left')          # Update the graph with the color information     for node in G.nodes:         node_id = node         color = merged_df.loc[merged_df['node'] == node_id, 'subregionColour'].values         G.nodes[node]['color'] = color[0] if len(color) &gt; 0 else '#000000'  # Default to black if no color found      return G  G_osiR = update_graph_colors(G_osiR, countries_df)  # Extract node colors and edge weights again after merging color information node_colors = [G_osiR.nodes[node]['color'] for node in subgraph.nodes] edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges]  # Normalize edge weights for plotting max_weight = max(edge_weights) min_weight = min(edge_weights) edge_thickness = [(weight - min_weight) / (max_weight - min_weight) * 5 + 1 for weight in edge_weights]  # Filter the subregions to only include those relevant to the top 25 edges and nodes relevant_nodes = set(subgraph.nodes) relevant_subregions = countries_df[countries_df['id'].isin(relevant_nodes)]['subregion'].unique()  # Create the legend for the relevant subregions relevant_subregion_colors = countries_df[countries_df['subregion'].isin(relevant_subregions)][['subregion', 'subregionColour']].drop_duplicates() handles = [mpatches.Patch(color=row['subregionColour'], label=row['subregion']) for idx, row in relevant_subregion_colors.iterrows()]  # Plot the graph with updated colors and arrows for directionality plt.figure(figsize=(17, 8)) pos = nx.spring_layout(subgraph, seed=42) nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, node_size=500) nx.draw_networkx_edges(subgraph, pos, width=edge_thickness, arrowstyle='-&gt;', arrowsize=10) nx.draw_networkx_labels(subgraph, pos, font_size=10)  # Add a title and subtitle as part of the main title plt.title('Top 25 Online Search Interest Flows\\nLine thickness represents the percentage of searches from country A to B')  # Add the legend for the relevant subregions inside the graph border plt.legend(handles=handles, title='Subregions', loc='upper right', bbox_to_anchor=(0.9, 0.9)) plt.savefig('top_25_online_search_interest_flows.png', format='png', bbox_inches='tight') plt.show() In\u00a0[30]: Copied! <pre>## foreign capital investment \n\n# fciN foreignCapitalInvestmentNormValue\n# fciR foreignCapitalInvestmentRawValue\n\n# Filter out rows with 'EU' as either countryA or countryB\nfiltered_df = network_power_df[(network_power_df['countryA'] != 'EU') &amp; (network_power_df['countryB'] != 'EU')]\n\n# Filter out rows with no value in foreignCapitalInvestmentRawValue column\nfiltered_df = filtered_df.dropna(subset=['foreignCapitalInvestmentRawValue', 'foreignCapitalInvestmentNormValue'])\n\n# Keep only the necessary columns\nfiltered_df = filtered_df[['countryA', 'countryB', 'foreignCapitalInvestmentRawValue', 'foreignCapitalInvestmentNormValue']]\n\n# Add back the filter: where countryA should be in countryB column\nfiltered_df = filtered_df[filtered_df['countryA'].isin(filtered_df['countryB'])]\n\n# Normalize the foreignCapitalInvestmentRawValue\n# Sum the raw values for each countryB\nraw_value_sum = filtered_df.groupby('countryB')['foreignCapitalInvestmentRawValue'].sum().reset_index()\nraw_value_sum.columns = ['countryB', 'sum_raw_value']\n\n# Merge the sums back to the original dataframe\nfiltered_df = filtered_df.merge(raw_value_sum, on='countryB')\n\n# Normalize the raw values\nfiltered_df['normalised_raw_value'] = filtered_df['foreignCapitalInvestmentRawValue'] / filtered_df['sum_raw_value']\n\n# Create graph G_fciN using the normalised_raw_value for weights\nG_fciN = nx.from_pandas_edgelist(\n    df=filtered_df,\n    source='countryA',\n    target='countryB',\n    edge_attr=['normalised_raw_value'],\n    create_using=nx.DiGraph()\n)\n\n# Rename edge attribute 'normalised_raw_value' to 'weight'\nfor u, v, d in G_fciN.edges(data=True):\n    d['weight'] = d.pop('normalised_raw_value')\n\n# Correct the index column and add attributes\ncountry_info = countries_df.set_index('id').to_dict('index')\n\n# Filter country_attributes_df for the year 2023\ncountry_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023].set_index('COUNTRY')\n\n# Create a dictionary for country attributes\ncountry_attributes_info = country_attributes_2023[['POPULATION (M)', 'GDP (B, USD)']].to_dict('index')\n\n# Update the graph with the correct attributes\nfor node in G_fciN.nodes():\n    if node in country_info:\n        G_fciN.nodes[node]['subregion'] = country_info[node].get('subregion')\n        G_fciN.nodes[node]['subregion_colour'] = country_info[node].get('subregionColour')\n    if node in country_attributes_info:\n        G_fciN.nodes[node]['population_2023'] = country_attributes_info[node].get('POPULATION (M)')\n        G_fciN.nodes[node]['GDP_2023'] = country_attributes_info[node].get('GDP (B, USD)')\n\n# Check the number of nodes and edges to confirm creation\nnum_nodes = G_fciN.number_of_nodes()\nnum_edges = G_fciN.number_of_edges()\n\n# Show a few edge details and node attributes to confirm everything has been added correctly\nedge_details = list(G_fciN.edges(data=True))[:5]\nnode_attributes = {node: G_fciN.nodes[node] for node in list(G_fciN.nodes)}\n\nnum_nodes, num_edges, edge_details, node_attributes\n</pre> ## foreign capital investment   # fciN foreignCapitalInvestmentNormValue # fciR foreignCapitalInvestmentRawValue  # Filter out rows with 'EU' as either countryA or countryB filtered_df = network_power_df[(network_power_df['countryA'] != 'EU') &amp; (network_power_df['countryB'] != 'EU')]  # Filter out rows with no value in foreignCapitalInvestmentRawValue column filtered_df = filtered_df.dropna(subset=['foreignCapitalInvestmentRawValue', 'foreignCapitalInvestmentNormValue'])  # Keep only the necessary columns filtered_df = filtered_df[['countryA', 'countryB', 'foreignCapitalInvestmentRawValue', 'foreignCapitalInvestmentNormValue']]  # Add back the filter: where countryA should be in countryB column filtered_df = filtered_df[filtered_df['countryA'].isin(filtered_df['countryB'])]  # Normalize the foreignCapitalInvestmentRawValue # Sum the raw values for each countryB raw_value_sum = filtered_df.groupby('countryB')['foreignCapitalInvestmentRawValue'].sum().reset_index() raw_value_sum.columns = ['countryB', 'sum_raw_value']  # Merge the sums back to the original dataframe filtered_df = filtered_df.merge(raw_value_sum, on='countryB')  # Normalize the raw values filtered_df['normalised_raw_value'] = filtered_df['foreignCapitalInvestmentRawValue'] / filtered_df['sum_raw_value']  # Create graph G_fciN using the normalised_raw_value for weights G_fciN = nx.from_pandas_edgelist(     df=filtered_df,     source='countryA',     target='countryB',     edge_attr=['normalised_raw_value'],     create_using=nx.DiGraph() )  # Rename edge attribute 'normalised_raw_value' to 'weight' for u, v, d in G_fciN.edges(data=True):     d['weight'] = d.pop('normalised_raw_value')  # Correct the index column and add attributes country_info = countries_df.set_index('id').to_dict('index')  # Filter country_attributes_df for the year 2023 country_attributes_2023 = country_attributes_df[country_attributes_df['Year'] == 2023].set_index('COUNTRY')  # Create a dictionary for country attributes country_attributes_info = country_attributes_2023[['POPULATION (M)', 'GDP (B, USD)']].to_dict('index')  # Update the graph with the correct attributes for node in G_fciN.nodes():     if node in country_info:         G_fciN.nodes[node]['subregion'] = country_info[node].get('subregion')         G_fciN.nodes[node]['subregion_colour'] = country_info[node].get('subregionColour')     if node in country_attributes_info:         G_fciN.nodes[node]['population_2023'] = country_attributes_info[node].get('POPULATION (M)')         G_fciN.nodes[node]['GDP_2023'] = country_attributes_info[node].get('GDP (B, USD)')  # Check the number of nodes and edges to confirm creation num_nodes = G_fciN.number_of_nodes() num_edges = G_fciN.number_of_edges()  # Show a few edge details and node attributes to confirm everything has been added correctly edge_details = list(G_fciN.edges(data=True))[:5] node_attributes = {node: G_fciN.nodes[node] for node in list(G_fciN.nodes)}  num_nodes, num_edges, edge_details, node_attributes Out[30]: <pre>(26,\n 362,\n [('AU', 'BA', {'weight': 0.008288524066872248}),\n  ('AU', 'BR', {'weight': 0.001320802853490291}),\n  ('AU', 'CB', {'weight': 0.006756833543662633}),\n  ('AU', 'CN', {'weight': 0.024316539304945796}),\n  ('AU', 'ID', {'weight': 0.02049699991551869})],\n {'AU': {'subregion': 'Australia and New Zealand',\n   'subregion_colour': '#cc0000',\n   'population_2023': 26.2,\n   'GDP_2023': 1688.0},\n  'BA': {'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41',\n   'population_2023': 171.2,\n   'GDP_2023': 446.0},\n  'BR': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 0.4,\n   'GDP_2023': 15.0},\n  'CB': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 16.8,\n   'GDP_2023': 31.0},\n  'CN': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 1434.1,\n   'GDP_2023': 18125.0},\n  'ID': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 275.5,\n   'GDP_2023': 1417.0},\n  'IN': {'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41',\n   'population_2023': 1417.2,\n   'GDP_2023': 3732.0},\n  'JP': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 124.0,\n   'GDP_2023': 4231.0},\n  'LA': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 7.5,\n   'GDP_2023': 14.0},\n  'MA': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 33.9,\n   'GDP_2023': 431.0},\n  'MO': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 3.4,\n   'GDP_2023': 19.0},\n  'MY': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 54.2,\n   'GDP_2023': 75.0},\n  'NP': {'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41',\n   'population_2023': 30.5,\n   'GDP_2023': 41.0},\n  'NZ': {'subregion': 'Australia and New Zealand',\n   'subregion_colour': '#cc0000',\n   'population_2023': 5.2,\n   'GDP_2023': 249.0},\n  'PG': {'subregion': 'Melanesia',\n   'subregion_colour': '#e93f7b',\n   'population_2023': 10.1,\n   'GDP_2023': 32.0},\n  'PH': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 115.6,\n   'GDP_2023': 436.0},\n  'PK': {'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41',\n   'population_2023': 235.8,\n   'GDP_2023': 341.0},\n  'RU': {'subregion': 'Eastern Europe',\n   'subregion_colour': '#fbe426',\n   'population_2023': 144.7,\n   'GDP_2023': 1862.0},\n  'SG': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 6.0,\n   'GDP_2023': 497.0},\n  'SK': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 51.8,\n   'GDP_2023': 1709.0},\n  'SL': {'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41',\n   'population_2023': 21.8,\n   'GDP_2023': 75.0},\n  'TH': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 71.7,\n   'GDP_2023': 512.0},\n  'TW': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 23.9,\n   'GDP_2023': 752.0},\n  'US': {'subregion': 'Northern America',\n   'subregion_colour': '#3392b3',\n   'population_2023': 338.3,\n   'GDP_2023': 26950.0},\n  'VN': {'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b',\n   'population_2023': 98.2,\n   'GDP_2023': 433.0},\n  'NK': {'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e',\n   'population_2023': 26.1,\n   'GDP_2023': 23.0}})</pre> In\u00a0[31]: Copied! <pre>node_size_log = {node: np.log(attr['GDP_2023'] + 1) * 100 for node, attr in G_fciN.nodes(data=True)}\n\n# Generate subregion colors from country_info\nsubregion_colours = {node: country_info[node]['subregionColour'] for node in G_fciN.nodes}\n\n# Generate edge widths for demonstration\nedge_weights = [d['weight'] if 'weight' in d else 1 for _, _, d in G_fciN.edges(data=True)]\nmax_edge_weight = max(edge_weights)\nedge_width = [10 * (weight / max_edge_weight) for weight in edge_weights]\n\n# Draw the graph with improved layout and legend\nplt.figure(figsize=(15, 10))\npos = nx.spring_layout(G_fciN, k=0.5)  # Increase k to separate nodes more\n\n# Draw nodes\nnodes = nx.draw_networkx_nodes(G_fciN, pos, node_size=[node_size_log[node] for node in G_fciN.nodes()],\n                               node_color=[subregion_colours[node] for node in G_fciN.nodes()])\n\n# Draw edges\nnx.draw_networkx_edges(G_fciN, pos, width=edge_width, alpha=0.5)\n\n# Draw labels\nnx.draw_networkx_labels(G_fciN, pos, font_size=8)\n\n# Create legend for subregions\nunique_subregions = {country_info[node]['subregion']: country_info[node]['subregionColour'] for node in G_fciN.nodes}\nlegend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=subregion) \n                  for subregion, color in unique_subregions.items()]\nplt.legend(handles=legend_handles, title=\"Subregions\", bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.title('Foreign Capital Investment Network')\n# Add title and explanation\nplt.title('Foreign Capital Investment Network\\nNode sizes represent log GDP (2023) and edge widths represent normalised FCI weights')\nplt.show()\n</pre> node_size_log = {node: np.log(attr['GDP_2023'] + 1) * 100 for node, attr in G_fciN.nodes(data=True)}  # Generate subregion colors from country_info subregion_colours = {node: country_info[node]['subregionColour'] for node in G_fciN.nodes}  # Generate edge widths for demonstration edge_weights = [d['weight'] if 'weight' in d else 1 for _, _, d in G_fciN.edges(data=True)] max_edge_weight = max(edge_weights) edge_width = [10 * (weight / max_edge_weight) for weight in edge_weights]  # Draw the graph with improved layout and legend plt.figure(figsize=(15, 10)) pos = nx.spring_layout(G_fciN, k=0.5)  # Increase k to separate nodes more  # Draw nodes nodes = nx.draw_networkx_nodes(G_fciN, pos, node_size=[node_size_log[node] for node in G_fciN.nodes()],                                node_color=[subregion_colours[node] for node in G_fciN.nodes()])  # Draw edges nx.draw_networkx_edges(G_fciN, pos, width=edge_width, alpha=0.5)  # Draw labels nx.draw_networkx_labels(G_fciN, pos, font_size=8)  # Create legend for subregions unique_subregions = {country_info[node]['subregion']: country_info[node]['subregionColour'] for node in G_fciN.nodes} legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=subregion)                    for subregion, color in unique_subregions.items()] plt.legend(handles=legend_handles, title=\"Subregions\", bbox_to_anchor=(1.05, 1), loc='upper left')  plt.title('Foreign Capital Investment Network') # Add title and explanation plt.title('Foreign Capital Investment Network\\nNode sizes represent log GDP (2023) and edge widths represent normalised FCI weights') plt.show() In\u00a0[32]: Copied! <pre>raw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()\n\n# Create a bar graph with country on the bottom and raw FCI value on y-axis, colored by subregion\nraw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()\nraw_fci_sum = raw_fci_sum.rename(columns={'countryA': 'Country', 'foreignCapitalInvestmentRawValue': 'Total Raw FCI'})\nraw_fci_sum = raw_fci_sum.merge(countries_df[['id', 'subregion']], left_on='Country', right_on='id')\n\nraw_fci_sum = raw_fci_sum.drop(columns=['id'])\nraw_fci_sum['subregion_color'] = raw_fci_sum['Country'].map(subregion_colours)\n\nraw_fci_sum\n</pre> raw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()  # Create a bar graph with country on the bottom and raw FCI value on y-axis, colored by subregion raw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index() raw_fci_sum = raw_fci_sum.rename(columns={'countryA': 'Country', 'foreignCapitalInvestmentRawValue': 'Total Raw FCI'}) raw_fci_sum = raw_fci_sum.merge(countries_df[['id', 'subregion']], left_on='Country', right_on='id')  raw_fci_sum = raw_fci_sum.drop(columns=['id']) raw_fci_sum['subregion_color'] = raw_fci_sum['Country'].map(subregion_colours)  raw_fci_sum Out[32]: Country Total Raw FCI subregion subregion_color 0 AU 6.075402e+10 Australia and New Zealand #cc0000 1 BA 1.529560e+09 Southern Asia #5d9c41 2 BR 1.590000e+08 Southeastern Asia #b6e24b 3 CB 6.443000e+08 Southeastern Asia #b6e24b 4 CN 3.670467e+11 Eastern Asia #7b965e 5 ID 3.894480e+09 Southeastern Asia #b6e24b 6 IN 4.698605e+10 Southern Asia #5d9c41 7 JP 3.647619e+11 Eastern Asia #7b965e 8 LA 3.414000e+08 Southeastern Asia #b6e24b 9 MA 5.336650e+10 Southeastern Asia #b6e24b 10 MO 1.767000e+08 Eastern Asia #7b965e 11 MY 2.091000e+08 Southeastern Asia #b6e24b 12 NP 9.240100e+08 Southern Asia #5d9c41 13 NZ 1.393034e+10 Australia and New Zealand #cc0000 14 PG 3.060000e+07 Melanesia #e93f7b 15 PH 1.389787e+10 Southeastern Asia #b6e24b 16 PK 1.123600e+09 Southern Asia #5d9c41 17 RU 2.100594e+10 Eastern Europe #fbe426 18 SG 1.748369e+11 Southeastern Asia #b6e24b 19 SK 2.009140e+11 Eastern Asia #7b965e 20 SL 9.606200e+08 Southern Asia #5d9c41 21 TH 6.689949e+10 Southeastern Asia #b6e24b 22 TW 1.501411e+11 Eastern Asia #7b965e 23 US 4.958655e+11 Northern America #3392b3 24 VN 1.277407e+10 Southeastern Asia #b6e24b In\u00a0[33]: Copied! <pre># Calculate where each country receives the most trade from and gives the most trade to\n\n# Group by countryB to find where each country receives the most trade from\nreceives_most_trade = filtered_df.loc[filtered_df.groupby('countryB')['normalised_raw_value'].idxmax()]\nreceives_most_trade = receives_most_trade[['countryB', 'countryA', 'normalised_raw_value']]\nreceives_most_trade = receives_most_trade.rename(columns={'countryB': 'Receiving Country', 'countryA': 'Top Sending Country', 'normalised_raw_value': 'Percentage Received'})\n\n# Group by countryA to find where each country gives the most trade to\ngives_most_trade = filtered_df.loc[filtered_df.groupby('countryA')['normalised_raw_value'].idxmax()]\ngives_most_trade = gives_most_trade[['countryA', 'countryB', 'normalised_raw_value']]\ngives_most_trade = gives_most_trade.rename(columns={'countryA': 'Sending Country', 'countryB': 'Top Receiving Country', 'normalised_raw_value': 'Percentage Sent'})\n\nraw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()\n\n# Create a bar graph with country on the bottom and raw FCI value on y-axis, coloured by subregion\nraw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()\nraw_fci_sum = raw_fci_sum.rename(columns={'countryA': 'Country', 'foreignCapitalInvestmentRawValue': 'Total Raw FCI'})\nraw_fci_sum = raw_fci_sum.merge(countries_df[['id', 'subregion']], left_on='Country', right_on='id')\n\nraw_fci_sum = raw_fci_sum.drop(columns=['id'])\nraw_fci_sum['subregion_colour'] = raw_fci_sum['Country'].map(subregion_colours)\n\n# Plot the corrected bar graph with y-axis in billions of USD\nplt.figure(figsize=(15, 10))\nbars = plt.bar(raw_fci_sum['Country'], raw_fci_sum['Total Raw FCI'] / 1e9, color=raw_fci_sum['subregion_colour'])  # Convert to billions\nplt.xlabel('Country')\nplt.ylabel('Total Raw FCI (Billions of USD)')\nplt.title('Total Raw FCI by Country, Coloured by Subregion')\n\n# Create legend for subregions\nlegend_handles_corrected = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colour, markersize=10, label=subregion) \n                            for subregion, colour in raw_fci_sum[['subregion', 'subregion_colour']].drop_duplicates().values]\nplt.legend(handles=legend_handles_corrected, title=\"Subregions\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=90)\nplt.show()\n\nreceives_most_trade, gives_most_trade\n</pre> # Calculate where each country receives the most trade from and gives the most trade to  # Group by countryB to find where each country receives the most trade from receives_most_trade = filtered_df.loc[filtered_df.groupby('countryB')['normalised_raw_value'].idxmax()] receives_most_trade = receives_most_trade[['countryB', 'countryA', 'normalised_raw_value']] receives_most_trade = receives_most_trade.rename(columns={'countryB': 'Receiving Country', 'countryA': 'Top Sending Country', 'normalised_raw_value': 'Percentage Received'})  # Group by countryA to find where each country gives the most trade to gives_most_trade = filtered_df.loc[filtered_df.groupby('countryA')['normalised_raw_value'].idxmax()] gives_most_trade = gives_most_trade[['countryA', 'countryB', 'normalised_raw_value']] gives_most_trade = gives_most_trade.rename(columns={'countryA': 'Sending Country', 'countryB': 'Top Receiving Country', 'normalised_raw_value': 'Percentage Sent'})  raw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index()  # Create a bar graph with country on the bottom and raw FCI value on y-axis, coloured by subregion raw_fci_sum = filtered_df.groupby('countryA')['foreignCapitalInvestmentRawValue'].sum().reset_index() raw_fci_sum = raw_fci_sum.rename(columns={'countryA': 'Country', 'foreignCapitalInvestmentRawValue': 'Total Raw FCI'}) raw_fci_sum = raw_fci_sum.merge(countries_df[['id', 'subregion']], left_on='Country', right_on='id')  raw_fci_sum = raw_fci_sum.drop(columns=['id']) raw_fci_sum['subregion_colour'] = raw_fci_sum['Country'].map(subregion_colours)  # Plot the corrected bar graph with y-axis in billions of USD plt.figure(figsize=(15, 10)) bars = plt.bar(raw_fci_sum['Country'], raw_fci_sum['Total Raw FCI'] / 1e9, color=raw_fci_sum['subregion_colour'])  # Convert to billions plt.xlabel('Country') plt.ylabel('Total Raw FCI (Billions of USD)') plt.title('Total Raw FCI by Country, Coloured by Subregion')  # Create legend for subregions legend_handles_corrected = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colour, markersize=10, label=subregion)                              for subregion, colour in raw_fci_sum[['subregion', 'subregion_colour']].drop_duplicates().values] plt.legend(handles=legend_handles_corrected, title=\"Subregions\", bbox_to_anchor=(1.05, 1), loc='upper left') plt.xticks(rotation=90) plt.show()  receives_most_trade, gives_most_trade Out[33]: <pre>(    Receiving Country Top Sending Country  Percentage Received\n 319                AU                  US             0.382781\n 320                BA                  US             0.289618\n 46                 BR                  CN             0.951353\n 47                 CB                  CN             0.270645\n 323                CN                  US             0.372402\n 48                 ID                  CN             0.235964\n 325                IN                  US             0.394642\n 326                JP                  US             0.520678\n 287                LA                  TH             0.402791\n 52                 MA                  CN             0.358137\n 115                MO                  JP             0.272899\n 116                MY                  JP             0.338748\n 55                 NK                  CN             1.000000\n 331                NP                  US             0.287661\n 332                NZ                  US             0.567850\n 58                 PG                  CN             0.515550\n 59                 PH                  CN             0.273997\n 60                 PK                  CN             0.673981\n 61                 RU                  CN             0.483832\n 337                SG                  US             0.450954\n 338                SK                  US             0.438147\n 64                 SL                  CN             0.322264\n 126                TH                  JP             0.499216\n 127                TW                  JP             0.532847\n 128                US                  JP             0.323224\n 272                VN                  SK             0.276239,\n     Sending Country Top Receiving Country  Percentage Sent\n 12               AU                    NZ         0.223254\n 25               BA                    IN         0.005256\n 35               BR                    SG         0.000799\n 39               CB                    LA         0.027722\n 55               CN                    NK         1.000000\n 76               ID                    MY         0.014102\n 86               IN                    BA         0.289577\n 127              JP                    TW         0.532847\n 131              LA                    TH         0.006073\n 135              MA                    CB         0.170306\n 153              MO                    CN         0.000405\n 160              MY                    TH         0.001140\n 167              NP                    SL         0.020089\n 168              NZ                    AU         0.045916\n 185              PG                    CB         0.001562\n 192              PH                    MA         0.037692\n 205              PK                    BA         0.004076\n 221              RU                    PK         0.088694\n 237              SG                    MO         0.222649\n 272              SK                    VN         0.276239\n 275              SL                    IN         0.002326\n 287              TH                    LA         0.402791\n 302              TW                    CN         0.158972\n 332              US                    NZ         0.567850\n 350              VN                    LA         0.098399)</pre> In\u00a0[34]: Copied! <pre>pagerank = nx.pagerank(G_fciN, weight='weight')\n\n# Sort and display the top 5 nodes by PageRank\nsorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\ntop_6_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[:6]]\n\n\ntop_6_pagerank\n\nbottom_5_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[-5:]]\n\nsorted_pagerank\n</pre> pagerank = nx.pagerank(G_fciN, weight='weight')  # Sort and display the top 5 nodes by PageRank sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True) top_6_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[:6]]   top_6_pagerank  bottom_5_pagerank = [(node, round(rank, 2)) for node, rank in sorted_pagerank[-5:]]  sorted_pagerank Out[34]: <pre>[('TH', 0.12270130156320815),\n ('LA', 0.11050607001286977),\n ('MY', 0.06236499236807532),\n ('CB', 0.058237664758859296),\n ('SL', 0.05758329401677341),\n ('IN', 0.0511434712036235),\n ('CN', 0.04490011477719617),\n ('SG', 0.04292667856007296),\n ('AU', 0.04052273550689094),\n ('BA', 0.034463124423883475),\n ('MA', 0.03337379592284281),\n ('VN', 0.03212258658015978),\n ('PH', 0.028827990518679514),\n ('RU', 0.027508029879278466),\n ('US', 0.02743722539403201),\n ('JP', 0.02700895144067068),\n ('ID', 0.026979701605581115),\n ('NP', 0.026909448327238746),\n ('PK', 0.022971722813098932),\n ('MO', 0.02061260506038507),\n ('SK', 0.020573383024858897),\n ('NZ', 0.02024417336137604),\n ('PG', 0.020214385804392342),\n ('TW', 0.014990098373258563),\n ('BR', 0.013684338281056453),\n ('NK', 0.011192116421637921)]</pre> In\u00a0[35]: Copied! <pre>hubs, authorities = nx.hits(G_fciN)\n# Sort and display the top 6 hubs and authorities\nsorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)\nsorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n\ntop_6_hubs = [(node, round(score, 2)) for node, score in sorted_hubs[:6]]\ntop_6_authorities = [(node, round(score, 2)) for node, score in sorted_authorities[:6]]\n\ntop_6_hubs, top_6_authorities\n</pre> hubs, authorities = nx.hits(G_fciN) # Sort and display the top 6 hubs and authorities sorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True) sorted_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)  top_6_hubs = [(node, round(score, 2)) for node, score in sorted_hubs[:6]] top_6_authorities = [(node, round(score, 2)) for node, score in sorted_authorities[:6]]  top_6_hubs, top_6_authorities Out[35]: <pre>([('CN', 0.38),\n  ('US', 0.2),\n  ('JP', 0.12),\n  ('SK', 0.05),\n  ('SG', 0.05),\n  ('AU', 0.04)],\n [('NK', 0.08),\n  ('BR', 0.08),\n  ('PK', 0.06),\n  ('RU', 0.05),\n  ('PG', 0.05),\n  ('SK', 0.04)])</pre> In\u00a0[36]: Copied! <pre>G_fciN_transformed = invert_weights(G_fciN)\n\n# Ocentrality measures as needed\npagerank = nx.pagerank(G_fciN, weight='weight')\ntop_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]\n\nhits_hubs, hits_authorities = nx.hits(G_fciN)\ntop_hits_hubs = sorted(hits_hubs.items(), key=lambda x: x[1], reverse=True)[:5]\ntop_hits_authorities = sorted(hits_authorities.items(), key=lambda x: x[1], reverse=True)[:5]\n\nbetweenness_centrality = nx.betweenness_centrality(G_fciN_transformed, weight='transformed_weight')\ntop_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n\nout_degree_centrality = dict(G_fciN.out_degree(weight='weight'))\ntop_out_degree_centrality = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n\n(top_pagerank, top_hits_hubs, top_hits_authorities, top_betweenness_centrality, top_out_degree_centrality)\n\n# Prepare data for visualization\ncentrality_measures = {\n    'PageRank': top_pagerank,\n    'HITS Hubs': top_hits_hubs,\n    'HITS Authorities': top_hits_authorities,\n    'Betweenness': top_betweenness_centrality,\n    'Weighted Out-Degree': top_out_degree_centrality\n}\n\n# Plot each centrality measure\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\naxes = axes.flatten()\n\nfor idx, (measure, data) in enumerate(centrality_measures.items()):\n    countries, values = zip(*data)\n    axes[idx].barh(countries, values, color='skyblue')\n    axes[idx].set_title(f'Top 5 Countries by {measure} Centrality')\n    axes[idx].invert_yaxis()\n    axes[idx].set_xlabel('Centrality Value')\n\nplt.tight_layout()\nplt.show()\n</pre>  G_fciN_transformed = invert_weights(G_fciN)  # Ocentrality measures as needed pagerank = nx.pagerank(G_fciN, weight='weight') top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]  hits_hubs, hits_authorities = nx.hits(G_fciN) top_hits_hubs = sorted(hits_hubs.items(), key=lambda x: x[1], reverse=True)[:5] top_hits_authorities = sorted(hits_authorities.items(), key=lambda x: x[1], reverse=True)[:5]  betweenness_centrality = nx.betweenness_centrality(G_fciN_transformed, weight='transformed_weight') top_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]  out_degree_centrality = dict(G_fciN.out_degree(weight='weight')) top_out_degree_centrality = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]  (top_pagerank, top_hits_hubs, top_hits_authorities, top_betweenness_centrality, top_out_degree_centrality)  # Prepare data for visualization centrality_measures = {     'PageRank': top_pagerank,     'HITS Hubs': top_hits_hubs,     'HITS Authorities': top_hits_authorities,     'Betweenness': top_betweenness_centrality,     'Weighted Out-Degree': top_out_degree_centrality }  # Plot each centrality measure fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20)) axes = axes.flatten()  for idx, (measure, data) in enumerate(centrality_measures.items()):     countries, values = zip(*data)     axes[idx].barh(countries, values, color='skyblue')     axes[idx].set_title(f'Top 5 Countries by {measure} Centrality')     axes[idx].invert_yaxis()     axes[idx].set_xlabel('Centrality Value')  plt.tight_layout() plt.show() diplomatic dialogues In\u00a0[37]: Copied! <pre># Filter the network_power_df for non-NaN and non-zero values in 'diplomaticDialoguesRawValue'\ndiplomatic_dialogues = network_power_df.dropna(subset=['diplomaticDialoguesRawValue'])\ndiplomatic_dialogues = diplomatic_dialogues[diplomatic_dialogues['diplomaticDialoguesRawValue'] &gt; 0]\n\n# Create an undirected graph for diplomatic dialogues\nG_didN = nx.from_pandas_edgelist(\n    df=diplomatic_dialogues,\n    source='countryA',\n    target='countryB',\n    edge_attr=['diplomaticDialoguesRawValue'],\n    create_using=nx.Graph()\n)\n\n# Rename edge attribute 'diplomaticDialoguesRawValue' to 'weight'\nfor u, v, d in G_didN.edges(data=True):\n    d['weight'] = d.pop('diplomaticDialoguesRawValue')\n\n# Add node attributes for regions\nfor node in G_didN.nodes():\n    if node in countries_df['id'].values:\n        node_data = countries_df[countries_df['id'] == node].iloc[0]\n        G_didN.nodes[node]['region'] = node_data['region']\n        G_didN.nodes[node]['region_colour'] = node_data['regionColour']\n        G_didN.nodes[node]['subregion'] = node_data['subregion']\n        G_didN.nodes[node]['subregion_colour'] = node_data['subregionColour']\n\n# Compute centrality measures for the new graph G_didN\n\n# Calculate PageRank\npagerank = nx.pagerank(G_didN, weight='weight')\ntop_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]\n\n# Calculate HITS (hubs and authorities) without weights\nhits_hubs, hits_authorities = nx.hits(G_didN)\ntop_hits_hubs = sorted(hits_hubs.items(), key=lambda x: x[1], reverse=True)[:5]\ntop_hits_authorities = sorted(hits_authorities.items(), key=lambda x: x[1], reverse=True)[:5]\n\n# Calculate Betweenness Centrality\nbetweenness_centrality = nx.betweenness_centrality(G_didN, weight='weight')\ntop_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n\n# Calculate Closeness Centrality with weights\ncloseness_centrality = nx.closeness_centrality(G_didN, distance='weight')\ntop_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n\n# Calculate Degree Centrality\ndegree = nx.degree(G_didN)\ntop_degree = sorted(degree, key=lambda x: x[1], reverse=True)[:5]\n\n(top_pagerank, top_hits_hubs, top_hits_authorities, top_betweenness_centrality, top_closeness_centrality, top_degree)\n</pre> # Filter the network_power_df for non-NaN and non-zero values in 'diplomaticDialoguesRawValue' diplomatic_dialogues = network_power_df.dropna(subset=['diplomaticDialoguesRawValue']) diplomatic_dialogues = diplomatic_dialogues[diplomatic_dialogues['diplomaticDialoguesRawValue'] &gt; 0]  # Create an undirected graph for diplomatic dialogues G_didN = nx.from_pandas_edgelist(     df=diplomatic_dialogues,     source='countryA',     target='countryB',     edge_attr=['diplomaticDialoguesRawValue'],     create_using=nx.Graph() )  # Rename edge attribute 'diplomaticDialoguesRawValue' to 'weight' for u, v, d in G_didN.edges(data=True):     d['weight'] = d.pop('diplomaticDialoguesRawValue')  # Add node attributes for regions for node in G_didN.nodes():     if node in countries_df['id'].values:         node_data = countries_df[countries_df['id'] == node].iloc[0]         G_didN.nodes[node]['region'] = node_data['region']         G_didN.nodes[node]['region_colour'] = node_data['regionColour']         G_didN.nodes[node]['subregion'] = node_data['subregion']         G_didN.nodes[node]['subregion_colour'] = node_data['subregionColour']  # Compute centrality measures for the new graph G_didN  # Calculate PageRank pagerank = nx.pagerank(G_didN, weight='weight') top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:5]  # Calculate HITS (hubs and authorities) without weights hits_hubs, hits_authorities = nx.hits(G_didN) top_hits_hubs = sorted(hits_hubs.items(), key=lambda x: x[1], reverse=True)[:5] top_hits_authorities = sorted(hits_authorities.items(), key=lambda x: x[1], reverse=True)[:5]  # Calculate Betweenness Centrality betweenness_centrality = nx.betweenness_centrality(G_didN, weight='weight') top_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]  # Calculate Closeness Centrality with weights closeness_centrality = nx.closeness_centrality(G_didN, distance='weight') top_closeness_centrality = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]  # Calculate Degree Centrality degree = nx.degree(G_didN) top_degree = sorted(degree, key=lambda x: x[1], reverse=True)[:5]  (top_pagerank, top_hits_hubs, top_hits_authorities, top_betweenness_centrality, top_closeness_centrality, top_degree) Out[37]: <pre>([('CN', 0.09738111293636326),\n  ('US', 0.07899367829033627),\n  ('AU', 0.07522430951216735),\n  ('ID', 0.06257830210433402),\n  ('SK', 0.05985312917156292)],\n [('US', 0.1035562376612992),\n  ('CN', 0.08395943232312468),\n  ('JP', 0.08151473754965999),\n  ('AU', 0.08080801392242692),\n  ('SK', 0.07727384741405997)],\n [('US', 0.1035562376612992),\n  ('CN', 0.0839594323231247),\n  ('JP', 0.08151473754966002),\n  ('AU', 0.08080801392242692),\n  ('SK', 0.07727384741405997)],\n [('ID', 0.13557754279098153),\n  ('TH', 0.10000427767226185),\n  ('CN', 0.09577765229939143),\n  ('CB', 0.0944856526872337),\n  ('SK', 0.08867657375562515)],\n [('ID', 0.5897435897435898),\n  ('TH', 0.575),\n  ('SK', 0.5609756097560976),\n  ('BR', 0.5227272727272727),\n  ('JP', 0.5111111111111111)],\n [('ID', 17), ('CN', 17), ('SK', 14), ('US', 14), ('AU', 13)])</pre> In\u00a0[38]: Copied! <pre># Check the number of nodes and edges to confirm creation\nnum_nodes = G_didN.number_of_nodes()\nnum_edges = G_didN.number_of_edges()\n\n# Show a few edge details and node attributes to confirm everything has been added correctly\nedge_details = list(G_didN.edges(data=True))[:5]\nnode_attributes = {node: G_didN.nodes[node] for node in list(G_didN.nodes)[:5]}\n\nnum_nodes, num_edges, edge_details, node_attributes\n\n# node_names = set(G_didN.nodes)\n# node_names2 = set(G_osiR.nodes)\n# node_names2 - node_names\n</pre> # Check the number of nodes and edges to confirm creation num_nodes = G_didN.number_of_nodes() num_edges = G_didN.number_of_edges()  # Show a few edge details and node attributes to confirm everything has been added correctly edge_details = list(G_didN.edges(data=True))[:5] node_attributes = {node: G_didN.nodes[node] for node in list(G_didN.nodes)[:5]}  num_nodes, num_edges, edge_details, node_attributes  # node_names = set(G_didN.nodes) # node_names2 = set(G_osiR.nodes) # node_names2 - node_names Out[38]: <pre>(24,\n 112,\n [('AU', 'CB', {'weight': 1.0}),\n  ('AU', 'ID', {'weight': 2.0}),\n  ('AU', 'IN', {'weight': 1.0}),\n  ('AU', 'JP', {'weight': 6.0}),\n  ('AU', 'MA', {'weight': 2.0})],\n {'AU': {'region': 'Oceania',\n   'region_colour': '#dc0000',\n   'subregion': 'Australia and New Zealand',\n   'subregion_colour': '#cc0000'},\n  'CB': {'region': 'Asia',\n   'region_colour': '#196c3d',\n   'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b'},\n  'ID': {'region': 'Asia',\n   'region_colour': '#196c3d',\n   'subregion': 'Southeastern Asia',\n   'subregion_colour': '#b6e24b'},\n  'IN': {'region': 'Asia',\n   'region_colour': '#196c3d',\n   'subregion': 'Southern Asia',\n   'subregion_colour': '#5d9c41'},\n  'JP': {'region': 'Asia',\n   'region_colour': '#196c3d',\n   'subregion': 'Eastern Asia',\n   'subregion_colour': '#7b965e'}})</pre> In\u00a0[39]: Copied! <pre>degree_dict = dict(nx.degree(G_didN))\n#degree_dict = dict(G_didN.degree(weight='weight'))\n\n# Convert the dictionary to a list of tuples and sort by degree\nsorted_degree_list = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n\nprint(sorted_degree_list)\n</pre>  degree_dict = dict(nx.degree(G_didN)) #degree_dict = dict(G_didN.degree(weight='weight'))  # Convert the dictionary to a list of tuples and sort by degree sorted_degree_list = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)  print(sorted_degree_list) <pre>[('ID', 17), ('CN', 17), ('SK', 14), ('US', 14), ('AU', 13), ('VN', 13), ('RU', 13), ('IN', 12), ('SG', 11), ('TH', 11), ('CB', 10), ('JP', 10), ('MA', 9), ('SL', 7), ('BR', 7), ('MO', 7), ('PK', 7), ('NZ', 6), ('BA', 6), ('LA', 6), ('PH', 5), ('NP', 4), ('MY', 3), ('PG', 2)]\n</pre> In\u00a0[40]: Copied! <pre># Prepare data for visualization\ncentrality_measures = {\n    'PageRank': top_pagerank,\n    'HITS Hubs': top_hits_hubs,\n    'HITS Authorities': top_hits_authorities,\n    'Betweenness': top_betweenness_centrality,\n    'Closeness': top_closeness_centrality,\n    'Degree': top_degree\n}\n\n# Plot each centrality measure\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\naxes = axes.flatten()\n\nfor idx, (measure, data) in enumerate(centrality_measures.items()):\n    countries, values = zip(*data)\n    axes[idx].barh(countries, values, color='skyblue')\n    axes[idx].set_title(f'Top 5 Countries by {measure} Centrality')\n    axes[idx].invert_yaxis()\n    axes[idx].set_xlabel('Centrality Value')\n\nplt.tight_layout()\nplt.show()\n</pre> # Prepare data for visualization centrality_measures = {     'PageRank': top_pagerank,     'HITS Hubs': top_hits_hubs,     'HITS Authorities': top_hits_authorities,     'Betweenness': top_betweenness_centrality,     'Closeness': top_closeness_centrality,     'Degree': top_degree }  # Plot each centrality measure fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20)) axes = axes.flatten()  for idx, (measure, data) in enumerate(centrality_measures.items()):     countries, values = zip(*data)     axes[idx].barh(countries, values, color='skyblue')     axes[idx].set_title(f'Top 5 Countries by {measure} Centrality')     axes[idx].invert_yaxis()     axes[idx].set_xlabel('Centrality Value')  plt.tight_layout() plt.show() In\u00a0[41]: Copied! <pre># Calculate weighted degree (this might not be needed if nodes are the same size)\nweighted_degree = dict(G_didN.degree(weight='weight'))\n\n# Define edge widths based on weights\nedge_widths = [d['weight'] for (u, v, d) in G_didN.edges(data=True)]\n\n# Define node colors based on region\nnode_colours = [G_didN.nodes[node]['subregion_colour'] for node in G_didN.nodes()]\n\n# Create a layout for the graph\npos = nx.spring_layout(G_didN, seed=42)\n\n# Plot the graph\nplt.figure(figsize=(20, 15))\nnx.draw_networkx_nodes(G_didN, pos, node_color=node_colours, node_size=1000, alpha=0.9)  # Adjust node_size as needed\nnx.draw_networkx_edges(G_didN, pos, width=edge_widths, edge_color='gray', alpha=0.7)\nnx.draw_networkx_labels(G_didN, pos, font_size=12, font_color='black')\n\n\nplt.title('Diplomatic Dialogues Network (Raw Values)\\nEdge thickness represents number of diplomatic dialogues held between country A and B')\nplt.axis('off')\nplt.show()\n</pre> # Calculate weighted degree (this might not be needed if nodes are the same size) weighted_degree = dict(G_didN.degree(weight='weight'))  # Define edge widths based on weights edge_widths = [d['weight'] for (u, v, d) in G_didN.edges(data=True)]  # Define node colors based on region node_colours = [G_didN.nodes[node]['subregion_colour'] for node in G_didN.nodes()]  # Create a layout for the graph pos = nx.spring_layout(G_didN, seed=42)  # Plot the graph plt.figure(figsize=(20, 15)) nx.draw_networkx_nodes(G_didN, pos, node_color=node_colours, node_size=1000, alpha=0.9)  # Adjust node_size as needed nx.draw_networkx_edges(G_didN, pos, width=edge_widths, edge_color='gray', alpha=0.7) nx.draw_networkx_labels(G_didN, pos, font_size=12, font_color='black')   plt.title('Diplomatic Dialogues Network (Raw Values)\\nEdge thickness represents number of diplomatic dialogues held between country A and B') plt.axis('off') plt.show() In\u00a0[42]: Copied! <pre># Community partition \n\n# Perform Louvain community detection on diplomatic dialogue netowrk\nlouvain_partition_didN = nx_comm.louvain_communities(G_didN, weight='weight', resolution=1, threshold=1e-07)\n\n# Convert partition to a dictionary for easier interpretation\nlouvain_communities_didN = {i: list(community) for i, community in enumerate(louvain_partition_didN)}\n\n# Display the resulting communities\nlouvain_communities_didN\n\n# Calculate Modularity\nmodularity_score_didN = nx_comm.modularity(G_didN, louvain_partition_didN, weight='weight')\nprint(f\"Modularity: {modularity_score_didN:.3f}\")\n\n# Normalise edge weights\nmax_weight_didN = max(nx.get_edge_attributes(G_didN, 'weight').values())\nmin_weight_didN = min(nx.get_edge_attributes(G_didN, 'weight').values())\nnorm_edge_weights_didN = [(d['weight'] - min_weight_didN) / (max_weight_didN - min_weight_didN) for _, _, d in G_didN.edges(data=True)]\n\nnum_communities_didN = len(louvain_communities_didN)\ncolours_didN = cm.rainbow(np.linspace(0, 1, num_communities_didN))\ncolour_map_didN = {i: colours_didN[i] for i in louvain_communities_didN.keys()}\n\n# Nosw colour assignment based on louvain_communities dictionary\nnode_community_map_didN = {node: community for community, nodes in louvain_communities_didN.items() for node in nodes}\nnode_colours_didN = [colour_map_didN[node_community_map_didN[node]] for node in G_didN.nodes]\n\n# Draw the graph with community colouring\npos = nx.spring_layout(G_didN, seed=42)\nplt.figure(figsize=(12, 8))\n\n# Draw nodes with community colours\nnx.draw_networkx_nodes(G_didN, pos, node_size=500, node_color=node_colours_didN, alpha=0.8)\nnx.draw_networkx_labels(G_didN, pos, font_size=10, font_color='white')\n\n# Draw edges with normalised weights\nnx.draw_networkx_edges(G_didN, pos, width=[w * 10 for w in norm_edge_weights_didN], alpha=0.5, edge_color='gray')\n\nplt.title(\"Louvain Communities in the Diplomatic Dialogue Network with Edge Weights\")\nplt.show()\nlouvain_communities_didN\n</pre> # Community partition   # Perform Louvain community detection on diplomatic dialogue netowrk louvain_partition_didN = nx_comm.louvain_communities(G_didN, weight='weight', resolution=1, threshold=1e-07)  # Convert partition to a dictionary for easier interpretation louvain_communities_didN = {i: list(community) for i, community in enumerate(louvain_partition_didN)}  # Display the resulting communities louvain_communities_didN  # Calculate Modularity modularity_score_didN = nx_comm.modularity(G_didN, louvain_partition_didN, weight='weight') print(f\"Modularity: {modularity_score_didN:.3f}\")  # Normalise edge weights max_weight_didN = max(nx.get_edge_attributes(G_didN, 'weight').values()) min_weight_didN = min(nx.get_edge_attributes(G_didN, 'weight').values()) norm_edge_weights_didN = [(d['weight'] - min_weight_didN) / (max_weight_didN - min_weight_didN) for _, _, d in G_didN.edges(data=True)]  num_communities_didN = len(louvain_communities_didN) colours_didN = cm.rainbow(np.linspace(0, 1, num_communities_didN)) colour_map_didN = {i: colours_didN[i] for i in louvain_communities_didN.keys()}  # Nosw colour assignment based on louvain_communities dictionary node_community_map_didN = {node: community for community, nodes in louvain_communities_didN.items() for node in nodes} node_colours_didN = [colour_map_didN[node_community_map_didN[node]] for node in G_didN.nodes]  # Draw the graph with community colouring pos = nx.spring_layout(G_didN, seed=42) plt.figure(figsize=(12, 8))  # Draw nodes with community colours nx.draw_networkx_nodes(G_didN, pos, node_size=500, node_color=node_colours_didN, alpha=0.8) nx.draw_networkx_labels(G_didN, pos, font_size=10, font_color='white')  # Draw edges with normalised weights nx.draw_networkx_edges(G_didN, pos, width=[w * 10 for w in norm_edge_weights_didN], alpha=0.5, edge_color='gray')  plt.title(\"Louvain Communities in the Diplomatic Dialogue Network with Edge Weights\") plt.show() louvain_communities_didN <pre>Modularity: 0.234\n</pre> Out[42]: <pre>{0: ['MA', 'ID', 'SG', 'BR'],\n 1: ['AU', 'US', 'PG', 'JP', 'SK', 'PH', 'NZ'],\n 2: ['BA', 'IN', 'NP', 'SL'],\n 3: ['LA', 'MY', 'CB', 'TH', 'VN', 'RU', 'MO', 'PK', 'CN']}</pre> In\u00a0[43]: Copied! <pre># Perform Louvain community detection on online search interest netowrk\n\nlouvain_partition_osiR = nx_comm.louvain_communities(G_osiR, weight='weight', resolution=1, threshold=1e-07)\n\n# Convert partition to a dictionary for easier interpretation\nlouvain_communities_osiR = {i: list(community) for i, community in enumerate(louvain_partition_osiR)}\n\n# Display the resulting communities\nlouvain_communities_osiR\n\n# Calculate Modularity\nmodularity_score_osiR = nx_comm.modularity(G_osiR, louvain_partition_osiR, weight='weight')\nprint(f\"Modularity: {modularity_score_osiR:.3f}\")\n\n# Normalize edge weights\nmax_weight_osiR = max(nx.get_edge_attributes(G_osiR, 'weight').values())\nmin_weight_osiR = min(nx.get_edge_attributes(G_osiR, 'weight').values())\nnorm_edge_weights_osiR = [(d['weight'] - min_weight_osiR) / (max_weight_osiR - min_weight_osiR) for _, _, d in G_osiR.edges(data=True)]\n\nnum_communities_osiR = len(louvain_communities_osiR)\ncolours_osiR= cm.rainbow(np.linspace(0, 1, num_communities_osiR))\ncolour_map_osiR = {i: colours_osiR[i] for i in louvain_communities_osiR.keys()}\n\n# Correct the node colours assignment based on louvain_communities dictionary\nnode_community_map_osiR = {node: community for community, nodes in louvain_communities_osiR.items() for node in nodes}\nnode_colours_osiR = [colour_map_osiR[node_community_map_osiR[node]] for node in G_osiR.nodes]\n\n# Draw the graph with community colouring\npos = nx.spring_layout(G_osiR, seed=42)\nplt.figure(figsize=(12, 8))\n\n# Draw nodes with community colours\nnx.draw_networkx_nodes(G_osiR, pos, node_size=500, node_color=node_colours_osiR, alpha=0.8)\nnx.draw_networkx_labels(G_osiR, pos, font_size=10, font_color='white')\n\n# Draw edges with normalised weights\nnx.draw_networkx_edges(G_osiR, pos, width=[w * 10 for w in norm_edge_weights_osiR], alpha=0.5, edge_color='gray')\n\nplt.title(\"Louvain Communities in the Online Search Index Network with Edge Weights\")\nplt.show()\nlouvain_communities_osiR\n</pre> # Perform Louvain community detection on online search interest netowrk  louvain_partition_osiR = nx_comm.louvain_communities(G_osiR, weight='weight', resolution=1, threshold=1e-07)  # Convert partition to a dictionary for easier interpretation louvain_communities_osiR = {i: list(community) for i, community in enumerate(louvain_partition_osiR)}  # Display the resulting communities louvain_communities_osiR  # Calculate Modularity modularity_score_osiR = nx_comm.modularity(G_osiR, louvain_partition_osiR, weight='weight') print(f\"Modularity: {modularity_score_osiR:.3f}\")  # Normalize edge weights max_weight_osiR = max(nx.get_edge_attributes(G_osiR, 'weight').values()) min_weight_osiR = min(nx.get_edge_attributes(G_osiR, 'weight').values()) norm_edge_weights_osiR = [(d['weight'] - min_weight_osiR) / (max_weight_osiR - min_weight_osiR) for _, _, d in G_osiR.edges(data=True)]  num_communities_osiR = len(louvain_communities_osiR) colours_osiR= cm.rainbow(np.linspace(0, 1, num_communities_osiR)) colour_map_osiR = {i: colours_osiR[i] for i in louvain_communities_osiR.keys()}  # Correct the node colours assignment based on louvain_communities dictionary node_community_map_osiR = {node: community for community, nodes in louvain_communities_osiR.items() for node in nodes} node_colours_osiR = [colour_map_osiR[node_community_map_osiR[node]] for node in G_osiR.nodes]  # Draw the graph with community colouring pos = nx.spring_layout(G_osiR, seed=42) plt.figure(figsize=(12, 8))  # Draw nodes with community colours nx.draw_networkx_nodes(G_osiR, pos, node_size=500, node_color=node_colours_osiR, alpha=0.8) nx.draw_networkx_labels(G_osiR, pos, font_size=10, font_color='white')  # Draw edges with normalised weights nx.draw_networkx_edges(G_osiR, pos, width=[w * 10 for w in norm_edge_weights_osiR], alpha=0.5, edge_color='gray')  plt.title(\"Louvain Communities in the Online Search Index Network with Edge Weights\") plt.show() louvain_communities_osiR <pre>Modularity: 0.226\n</pre> Out[43]: <pre>{0: ['BA', 'PG', 'SL', 'NZ', 'AU', 'IN', 'PK', 'NP'],\n 1: ['MA', 'ID', 'SG', 'BR'],\n 2: ['TW',\n  'LA',\n  'NK',\n  'US',\n  'JP',\n  'MY',\n  'PH',\n  'CB',\n  'TH',\n  'VN',\n  'RU',\n  'MO',\n  'SK',\n  'CN']}</pre> In\u00a0[44]: Copied! <pre># Perform Louvain community detection for fciN network\nlouvain_partition_fciN = nx_comm.louvain_communities(G_fciN, weight='weight', resolution=1, threshold=1e-07)\n\n# Convert partition to a dictionary for easier interpretation\nlouvain_communities_fciN = {i: list(community) for i, community in enumerate(louvain_partition_fciN)}\n\n# Display the resulting communities\nprint(louvain_communities_fciN)\n\n# Calculate Modularity\nmodularity_score_fciN = nx_comm.modularity(G_fciN, louvain_partition_fciN, weight='weight')\nprint(f\"Modularity (fciN): {modularity_score_fciN:.3f}\")\n\n# Normalise edge weights for fciN network\nmax_weight_fciN = max(nx.get_edge_attributes(G_fciN, 'weight').values())\nmin_weight_fciN = min(nx.get_edge_attributes(G_fciN, 'weight').values())\nnorm_edge_weights_fciN = [(d['weight'] - min_weight_fciN) / (max_weight_fciN - min_weight_fciN) for _, _, d in G_fciN.edges(data=True)]\n\nnum_communities_fciN = len(louvain_communities_fciN)\ncolours_fciN = cm.rainbow(np.linspace(0, 1, num_communities_fciN))\ncolour_map_fciN = {i: colours_fciN[i] for i in louvain_communities_fciN.keys()}\n\n# Correct the node colours assignment based on louvain_communities dictionary\nnode_community_map_fciN = {node: community for community, nodes in louvain_communities_fciN.items() for node in nodes}\nnode_colours_fciN = [colour_map_fciN[node_community_map_fciN[node]] for node in G_fciN.nodes]\n\n# Draw the graph with community colouring for fciN network\npos_fciN = nx.spring_layout(G_fciN, seed=42)\nplt.figure(figsize=(12, 8))\n\n# Draw nodes with community colours\nnx.draw_networkx_nodes(G_fciN, pos_fciN, node_size=500, node_color=node_colours_fciN, alpha=0.8)\nnx.draw_networkx_labels(G_fciN, pos_fciN, font_size=10, font_color='white')\n\n# Draw edges with normalized weights\nnx.draw_networkx_edges(G_fciN, pos_fciN, width=[w * 10 for w in norm_edge_weights_fciN], alpha=0.5, edge_color='gray')\n\nplt.title(\"Louvain Communities in the Foreign Capital Investment Network with Edge Weights\")\nplt.show()\n</pre> # Perform Louvain community detection for fciN network louvain_partition_fciN = nx_comm.louvain_communities(G_fciN, weight='weight', resolution=1, threshold=1e-07)  # Convert partition to a dictionary for easier interpretation louvain_communities_fciN = {i: list(community) for i, community in enumerate(louvain_partition_fciN)}  # Display the resulting communities print(louvain_communities_fciN)  # Calculate Modularity modularity_score_fciN = nx_comm.modularity(G_fciN, louvain_partition_fciN, weight='weight') print(f\"Modularity (fciN): {modularity_score_fciN:.3f}\")  # Normalise edge weights for fciN network max_weight_fciN = max(nx.get_edge_attributes(G_fciN, 'weight').values()) min_weight_fciN = min(nx.get_edge_attributes(G_fciN, 'weight').values()) norm_edge_weights_fciN = [(d['weight'] - min_weight_fciN) / (max_weight_fciN - min_weight_fciN) for _, _, d in G_fciN.edges(data=True)]  num_communities_fciN = len(louvain_communities_fciN) colours_fciN = cm.rainbow(np.linspace(0, 1, num_communities_fciN)) colour_map_fciN = {i: colours_fciN[i] for i in louvain_communities_fciN.keys()}  # Correct the node colours assignment based on louvain_communities dictionary node_community_map_fciN = {node: community for community, nodes in louvain_communities_fciN.items() for node in nodes} node_colours_fciN = [colour_map_fciN[node_community_map_fciN[node]] for node in G_fciN.nodes]  # Draw the graph with community colouring for fciN network pos_fciN = nx.spring_layout(G_fciN, seed=42) plt.figure(figsize=(12, 8))  # Draw nodes with community colours nx.draw_networkx_nodes(G_fciN, pos_fciN, node_size=500, node_color=node_colours_fciN, alpha=0.8) nx.draw_networkx_labels(G_fciN, pos_fciN, font_size=10, font_color='white')  # Draw edges with normalized weights nx.draw_networkx_edges(G_fciN, pos_fciN, width=[w * 10 for w in norm_edge_weights_fciN], alpha=0.5, edge_color='gray')  plt.title(\"Louvain Communities in the Foreign Capital Investment Network with Edge Weights\") plt.show() <pre>{0: ['BA', 'IN', 'NP', 'SL'], 1: ['VN', 'MA', 'LA', 'MY', 'PH', 'CB', 'TH'], 2: ['TW', 'ID', 'US', 'JP', 'SG', 'NZ', 'AU', 'MO', 'SK'], 3: ['RU', 'BR', 'NK', 'PK', 'PG', 'CN']}\nModularity (fciN): 0.204\n</pre> In\u00a0[45]: Copied! <pre># Define the groupings\ngrouping_1 = louvain_communities_didN\n\ngrouping_2 = louvain_communities_osiR\n\ngrouping_3 = louvain_communities_fciN\n\n# Create a function to assign labels to each node\ndef create_labels(grouping):\n    labels = {}\n    for community, nodes in grouping.items():\n        for node in nodes:\n            labels[node] = community\n    return labels\n\n# Create labels for each grouping\nlabels_1 = create_labels(grouping_1)\nlabels_2 = create_labels(grouping_2)\nlabels_3 = create_labels(grouping_3)\n\n# Ensure all nodes are considered by filling in missing nodes with a unique label\nall_nodes = set(labels_1.keys()).union(labels_2.keys()).union(labels_3.keys())\nfor node in all_nodes:\n    if node not in labels_1:\n        labels_1[node] = -1\n    if node not in labels_2:\n        labels_2[node] = -1\n    if node not in labels_3:\n        labels_3[node] = -1\n\n# Create lists of labels for ARI calculation\nlabels_list_1 = [labels_1[node] for node in sorted(all_nodes)]\nlabels_list_2 = [labels_2[node] for node in sorted(all_nodes)]\nlabels_list_3 = [labels_3[node] for node in sorted(all_nodes)]\n\n# Calculate ARI between groupings\nari_1_2 = adjusted_rand_score(labels_list_1, labels_list_2)\nari_1_3 = adjusted_rand_score(labels_list_1, labels_list_3)\nari_2_3 = adjusted_rand_score(labels_list_2, labels_list_3)\n\nprint(f\"ARI between grouping 1 and grouping 2: {ari_1_2:.3f}\")\nprint(f\"ARI between grouping 1 and grouping 3: {ari_1_3:.3f}\")\nprint(f\"ARI between grouping 2 and grouping 3: {ari_2_3:.3f}\")\n</pre> # Define the groupings grouping_1 = louvain_communities_didN  grouping_2 = louvain_communities_osiR  grouping_3 = louvain_communities_fciN  # Create a function to assign labels to each node def create_labels(grouping):     labels = {}     for community, nodes in grouping.items():         for node in nodes:             labels[node] = community     return labels  # Create labels for each grouping labels_1 = create_labels(grouping_1) labels_2 = create_labels(grouping_2) labels_3 = create_labels(grouping_3)  # Ensure all nodes are considered by filling in missing nodes with a unique label all_nodes = set(labels_1.keys()).union(labels_2.keys()).union(labels_3.keys()) for node in all_nodes:     if node not in labels_1:         labels_1[node] = -1     if node not in labels_2:         labels_2[node] = -1     if node not in labels_3:         labels_3[node] = -1  # Create lists of labels for ARI calculation labels_list_1 = [labels_1[node] for node in sorted(all_nodes)] labels_list_2 = [labels_2[node] for node in sorted(all_nodes)] labels_list_3 = [labels_3[node] for node in sorted(all_nodes)]  # Calculate ARI between groupings ari_1_2 = adjusted_rand_score(labels_list_1, labels_list_2) ari_1_3 = adjusted_rand_score(labels_list_1, labels_list_3) ari_2_3 = adjusted_rand_score(labels_list_2, labels_list_3)  print(f\"ARI between grouping 1 and grouping 2: {ari_1_2:.3f}\") print(f\"ARI between grouping 1 and grouping 3: {ari_1_3:.3f}\") print(f\"ARI between grouping 2 and grouping 3: {ari_2_3:.3f}\")  <pre>ARI between grouping 1 and grouping 2: 0.327\nARI between grouping 1 and grouping 3: 0.231\nARI between grouping 2 and grouping 3: 0.098\n</pre>"}]}